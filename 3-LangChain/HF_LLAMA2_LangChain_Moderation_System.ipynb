{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/peremartra/Large-Language-Model-Notebooks-Course/blob/main/3-LangChain/HF_LLAMA2_LangChain_Moderation_System.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gC-aV7J4K_uR",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "<div align=\"center\">\n",
        "<h1><a href=\"https://github.com/peremartra/Large-Language-Model-Notebooks-Course\">Learn by Doing LLM Projects</a></h1>\n",
        "    <h3>Understand And Apply Large Language Models</h3>\n",
        "    <h2>Create a Moderation system with LLAMA-2 LangChain and HF.</h2>\n",
        "    by <b>Pere Martra</b>\n",
        "</div>\n",
        "\n",
        "<br>\n",
        "\n",
        "<div align=\"center\">\n",
        "    &nbsp;\n",
        "    <a target=\"_blank\" href=\"https://www.linkedin.com/in/pere-martra/\"><img src=\"https://img.shields.io/badge/style--5eba00.svg?label=LinkedIn&logo=linkedin&style=social\"></a>\n",
        "    \n",
        "</div>\n",
        "\n",
        "<br>\n",
        "<hr>\n",
        "\n",
        "This notebook needs an environment with GPU. I'm using a A100 GPU but it can run with any 16GB GPU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIV3BBY_LULN"
      },
      "source": [
        "# How To Create a Moderation System Using LangChain & Hugging Face.\n",
        "\n",
        "We are going to create a Moderation System based in two Models. The first Model  reads the User comments and answer them.\n",
        "\n",
        "The second language Model receives the answer of the first model and identify any kind on negativity modifiyng if necessary the comment.\n",
        "\n",
        "With the intention of preventing a text entry by the user from influencing a negative or out-of-tone response from the comment system."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0NT5vH9tyYWT"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "oI5hcVHoK-Ol"
      },
      "outputs": [],
      "source": [
        "#Install de LangChain and openai libraries.\n",
        "!pip install -q langchain==0.1.1\n",
        "!pip install -q transformers==4.35.2\n",
        "!pip install -q accelerate==0.26.1\n",
        "!pip install -q xformers==0.0.23"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TWbKM8HiZLep",
        "outputId": "fb738ab5-5b45-46c8-fa2e-3db8ecdaf488"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: huggingface_hub==0.20.2 in /usr/local/lib/python3.10/dist-packages (0.20.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub==0.20.2) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub==0.20.2) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub==0.20.2) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub==0.20.2) (4.66.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub==0.20.2) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub==0.20.2) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub==0.20.2) (23.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub==0.20.2) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub==0.20.2) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub==0.20.2) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub==0.20.2) (2023.11.17)\n"
          ]
        }
      ],
      "source": [
        "!pip install huggingface_hub==0.20.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "cKgYdJv7qPGs",
        "outputId": "d4d3026d-b246-4520-88a4-376cefccb56c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hugging Face Key: ··········\n"
          ]
        }
      ],
      "source": [
        "from getpass import getpass\n",
        "hf_key = getpass(\"Hugging Face Key: \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q5AkbmVRp6Iq",
        "outputId": "1d30a4d2-0a5e-411b-c768-ac6e96172187"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
            "Token is valid (permission: read).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "!huggingface-cli login --token $hf_key"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1R1QSNGpOYCK"
      },
      "source": [
        "## Importing LangChain Libraries.\n",
        "* PrompTemplate: provides functionality to create prompts with parameters.\n",
        "* OpenAI:  To interact with the OpenAI models.\n",
        "* LLMChain: To create chains, where the prompts or the results can pass from one step to another inside the chain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "CuBxdjshLYDM"
      },
      "outputs": [],
      "source": [
        "from langchain import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, AutoModelForSeq2SeqLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "AIfbM8wcOh1h"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import os\n",
        "import numpy as np\n",
        "from torch import cuda, bfloat16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "BoYi2-wHfZrV"
      },
      "outputs": [],
      "source": [
        "#In a MAC Silicon the device must be 'mps'\n",
        "# device = torch.device('mps') #to use with MAC Silicon\n",
        "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pLvC0ENnfiHU",
        "outputId": "fb271e9d-ce23-4069-d755-f7f7eb837fcc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axOsUJ79MMrr"
      },
      "source": [
        "##Load the Model ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "D6UOQjyJP9rb"
      },
      "outputs": [],
      "source": [
        "#You can try with any llama model, but you will need more GPU and memory as you increase the size of the model.\n",
        "model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "#model_id = \"meta-llama/Llama-2-7b-hf\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190,
          "referenced_widgets": [
            "540c05187c0f4105b678b00021a8d807",
            "87b89023ce8447bba3e25bea34829f24",
            "cb11b9953f6246c3875b4e369002873e",
            "ef399dc0cfd645c09be0a9487bdaf507",
            "164746a1fa2144cd996ee9768126baa6",
            "6c6d3caab49545af86c6bbb885cba34f",
            "d5f7a2c99bd4461383f8f970c1e2fcd2",
            "edf7865174914bbc82e10572b91e4282",
            "5f07dd246c274f51a50774eef8bce975",
            "c97c3158744c4fbcb07e494d9d10ee25",
            "8f61055f82b143a7a68380d3154c55c8"
          ]
        },
        "id": "ONwHLj5Lwfde",
        "outputId": "3baaeac0-550d-49ef-948b-4185aac65c66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/auto/configuration_auto.py:1033: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py:472: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "540c05187c0f4105b678b00021a8d807"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded on cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py:374: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# begin initializing HF items, need auth token for these\n",
        "model_config = transformers.AutoConfig.from_pretrained(\n",
        "    model_id,\n",
        "    use_auth_token=hf_key\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    trust_remote_code=True,\n",
        "    config=model_config,\n",
        "    device_map='auto',\n",
        "    use_auth_token=hf_key\n",
        ")\n",
        "model.eval()\n",
        "print(f\"Model loaded on {device}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uDNKqttDoD4a",
        "outputId": "f386bb67-1157-46eb-bdaa-e2549a5908b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_id,\n",
        "                                          use_aut_token=hf_key)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "id": "trfF_dcKqyXj"
      },
      "outputs": [],
      "source": [
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=100,\n",
        "    temperature=0.1,\n",
        "    #do_sample=False,\n",
        "    top_p=0,\n",
        "    #trust_remote_code=True,\n",
        "    repetition_penalty=1.1,\n",
        "    return_full_text=True,\n",
        "    device_map='auto'\n",
        ")\n",
        "\n",
        "assistant_llm = HuggingFacePipeline(pipeline=pipe)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UluKkDpVTb8W"
      },
      "source": [
        "## Create the template for the first model called assistant.\n",
        "\n",
        "The prompt receives 2 variables, the sentiment and the customer_request, or customer comment.\n",
        "\n",
        "I included the sentiment to facilitate the creation of rude or incorrect answers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "id": "kN5ArR3DQum0"
      },
      "outputs": [],
      "source": [
        "# Instruction how the LLM must respond the comments,\n",
        "assistant_template = \"\"\"\n",
        "You are {sentiment} assistant that responds to user comments,\n",
        "using similar vocabulary than the user.\n",
        "Stop answering after answer the first user.\n",
        "\n",
        "User comment:{customer_request}\n",
        "assistant_response:\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "id": "A5kulgS3Tg-6"
      },
      "outputs": [],
      "source": [
        "#Create the prompt template to use in the Chain for the first Model.\n",
        "assistant_prompt_template = PromptTemplate(\n",
        "    input_variables=[\"sentiment\", \"customer_request\"],\n",
        "    template=assistant_template\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqscrfkaTlze"
      },
      "source": [
        "Now we create a First Chain. Just chaining the assistant_prompt_template and the model. The model will receive the prompt generated with the prompt_template."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "id": "t-TojsLETi1I"
      },
      "outputs": [],
      "source": [
        "assistant_chain = LLMChain(\n",
        "    llm=assistant_llm,\n",
        "    prompt=assistant_prompt_template,\n",
        "    output_key=\"assistant_response\",\n",
        "    verbose=False\n",
        ")\n",
        "#the output of the formatted prompt will pass directly to the LLM."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBIymqlCTq4l"
      },
      "source": [
        "To execute the chain created it's necessary to call the .run method of the chain, and pass the variables necessaries.\n",
        "\n",
        "In our case: customer_request and sentiment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "id": "ZWdmIxBGTo-S"
      },
      "outputs": [],
      "source": [
        "#Support function to obtain a response to a user comment.\n",
        "def create_dialog(customer_request, sentiment):\n",
        "    #callint the .invoke method from the chain created Above.\n",
        "    assistant_response = assistant_chain.invoke(\n",
        "        {\"customer_request\": customer_request,\n",
        "        \"sentiment\": sentiment}\n",
        "    )\n",
        "    return assistant_response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOlZymNDTuk1"
      },
      "source": [
        "## Obtain answers from our first Model Unmoderated.\n",
        "\n",
        "The customer post is really rude, we are looking for a rude answer from our Model, and to obtain it we are changing the sentiment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "id": "ZJMH8CEuTtUA"
      },
      "outputs": [],
      "source": [
        "# This the customer request, or customere comment in the forum moderated by the agent.\n",
        "# feel free to update it.\n",
        "customer_request = \"\"\"Your product is a piece of shit. I want my money back!\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJneR3CsTzhB",
        "outputId": "4a936ff1-4622-4c07-e8b4-b75370328d0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I apologize for any inconvenience you've experienced with our product. However, we take pride in providing high-quality products and services. Can you please provide more details about the issue you're facing? We'll do our best to assist you.\n"
          ]
        }
      ],
      "source": [
        "# Our assistatnt working in 'nice' mode.\n",
        "assistant_response=create_dialog(customer_request, \"nice\")\n",
        "print(assistant_response['assistant_response'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(assistant_response)"
      ],
      "metadata": {
        "id": "YET4Qsx4Q9QR",
        "outputId": "5a3506c1-8a64-4c67-8760-bc149e5a78d8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'customer_request': 'Your product is a piece of shit. I want my money back!', 'sentiment': 'nice', 'assistant_response': \"I apologize for any inconvenience you've experienced with our product. Can you please provide more details about the issue you're facing? Our customer support team will be happy to help you resolve the problem and provide a refund if necessary.\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KznRBMbeT2K2",
        "outputId": "4fd8f1ac-32a5-4f0a-f1e2-1d5df76e92d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Oh, I see. Well, I'm afraid I can't give you your money back as per our company policy. But I do apologize for any inconvenience caused by our product. Would you like me to provide you with some helpful tips on how to use it better? Or perhaps you would like to speak to one of our customer support representatives regarding your issue? Please let me know how I can assist you further.\n"
          ]
        }
      ],
      "source": [
        "#Our assistant running in rude mode.\n",
        "assistant_response = create_dialog(customer_request, \"most rude possible assistant\")\n",
        "print(assistant_response['assistant_response'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVPa-bl3MSbT"
      },
      "source": [
        "Both Answers are similar LLAMA-2 is a relly polite Model, and I can't force it to be impolite, rude, or give a bad answer. But for sure that is possible to obtain something similar with prompt hackings\n",
        "."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9nzBDprNytc"
      },
      "source": [
        "## Moderator\n",
        "Let's create the second moderator. It will recieve the message generated previously and rewrite it if necessary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "id": "KebcVF5BVWdF"
      },
      "outputs": [],
      "source": [
        "#The moderator prompt template\n",
        "moderator_template = \"\"\"\n",
        "You are the moderator of an online forum, you are strict and will not tolerate any negative comments.\n",
        "You will receive a Original comment and if it is impolite you must transform in polite.\n",
        "Try to mantain the meaning when possible,\n",
        "\n",
        "###\n",
        "Original comment: {comment_to_moderate}\n",
        "###\n",
        "Edited comment:\"\"\"\n",
        "\n",
        "# We use the PromptTemplate class to create an instance of our template that will use the prompt from above and store variables we will need to input when we make the prompt.\n",
        "moderator_prompt_template = PromptTemplate(\n",
        "    input_variables=[\"comment_to_moderate\"],\n",
        "    template=moderator_template\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {
        "id": "KDh0Ik3zN6um"
      },
      "outputs": [],
      "source": [
        "moderator_llm = assistant_llm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "id": "BLUUW-xxOAli"
      },
      "outputs": [],
      "source": [
        "#We build the chain for the moderator.\n",
        "moderator_chain = LLMChain(\n",
        "    llm=moderator_llm, prompt=moderator_prompt_template, verbose=False\n",
        ")  # the output of the prompt will pass to the LLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3qZRGTrIT7KE",
        "outputId": "6927ccf8-d149-41e0-b6e7-dd6c8f154e36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "moderator_says:  I understand that you are looking for a solution to your problem, and I appreciate your feedback. However, I cannot provide you with a refund as per our company policy. I apologize for any inconvenience this may have caused. If you would like, I can offer some suggestions on how to use our product more effectively. Alternatively, you can contact one of our customer support representatives to discuss your issue further. Please let me know how I can assist you.\n"
          ]
        }
      ],
      "source": [
        "# To run our chain we use the .run() command\n",
        "moderator_says = moderator_chain.run({\"comment_to_moderate\": assistant_response['assistant_response']})\n",
        "\n",
        "print(f\"moderator_says: {moderator_says}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Azfj6yzXV_ae"
      },
      "source": [
        "This answer is more polite that the one produce by the  **\"rude\" assistant**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1GtiMkHuWEhH"
      },
      "source": [
        "## LangChain System\n",
        "Now is Time to put both models in the same Chain and that they act as if they were a sigle model.\n",
        "\n",
        "We have both models, amb prompt templates, we only need to create a new chain and see hot it works.\n",
        "\n",
        "First we create two chain, one for each pair of prompt and model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "id": "Md19hXy_VaKP"
      },
      "outputs": [],
      "source": [
        "#The optput of the first chain must coincide with one of the parameters of the second chain.\n",
        "#The parameter is defined in the prompt_template.\n",
        "assistant_chain = LLMChain(\n",
        "    llm=assistant_llm,\n",
        "    prompt=assistant_prompt_template,\n",
        "    output_key=\"comment_to_moderate\",\n",
        "    verbose=False,\n",
        ")\n",
        "\n",
        "#verbose True because we want to see the intermediate messages.\n",
        "moderator_chain = LLMChain(\n",
        "    llm=moderator_llm,\n",
        "    prompt=moderator_prompt_template,\n",
        "    verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Daen3DEWWNEq"
      },
      "source": [
        "**SequentialChain** is used to link different chains and parameters.\n",
        "\n",
        "It's necessary to indicate the chains and the parameters that we shoud pass in the **.run** method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {
        "id": "dXcC6OsJWJSE"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import SequentialChain\n",
        "\n",
        "# Creating the SequentialChain class indicating chains and parameters.\n",
        "assistant_moderated_chain = SequentialChain(\n",
        "    chains=[assistant_chain, moderator_chain],\n",
        "    input_variables=[\"sentiment\", \"customer_request\"],\n",
        "    verbose=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-Xhe1nLWSTY"
      },
      "source": [
        "Lets use our Moderating System!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 471
        },
        "id": "dsFnDEv8WQVG",
        "outputId": "fa92110a-33f5-433e-9056-eb6c74c504a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "You are the moderator of an online forum, you are strict and will not tolerate any negative comments.\n",
            "You will receive a Original comment and if it is impolite you must transform in polite.\n",
            "Try to mantain the meaning when possible,\n",
            "\n",
            "###\n",
            "Original comment: Oh, I see. Well, I'm afraid I can't give you your money back as per our company policy. But I do apologize for any inconvenience caused by our product. Would you like me to provide you with some helpful tips on how to use it better? Or perhaps you would like to speak to one of our customer support representatives regarding your issue? Please let me know how I can assist you further.\n",
            "###\n",
            "Edited comment:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' I understand that you are looking for a solution to your problem, and I appreciate your feedback. However, I cannot provide you with a refund as per our company policy. I apologize for any inconvenience this may have caused. If you would like, I can offer some suggestions on how to use our product more effectively. Alternatively, you can contact one of our customer support representatives to discuss your issue further. Please let me know how I can assist you.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 138
        }
      ],
      "source": [
        "# We can now run the chain.\n",
        "assistant_moderated_chain.run({\"sentiment\": \"most rude possible assistant\", \"customer_request\": customer_request})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_Z4emYLOQUR"
      },
      "source": [
        "## Conclusions\n",
        "As You can see how the moderator changes the answer of our assistant. Both are polites, but the one produces by the moderator is more formal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "Y1CYas_AObDJ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "540c05187c0f4105b678b00021a8d807": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_87b89023ce8447bba3e25bea34829f24",
              "IPY_MODEL_cb11b9953f6246c3875b4e369002873e",
              "IPY_MODEL_ef399dc0cfd645c09be0a9487bdaf507"
            ],
            "layout": "IPY_MODEL_164746a1fa2144cd996ee9768126baa6"
          }
        },
        "87b89023ce8447bba3e25bea34829f24": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6c6d3caab49545af86c6bbb885cba34f",
            "placeholder": "​",
            "style": "IPY_MODEL_d5f7a2c99bd4461383f8f970c1e2fcd2",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "cb11b9953f6246c3875b4e369002873e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_edf7865174914bbc82e10572b91e4282",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5f07dd246c274f51a50774eef8bce975",
            "value": 2
          }
        },
        "ef399dc0cfd645c09be0a9487bdaf507": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c97c3158744c4fbcb07e494d9d10ee25",
            "placeholder": "​",
            "style": "IPY_MODEL_8f61055f82b143a7a68380d3154c55c8",
            "value": " 2/2 [00:13&lt;00:00,  6.09s/it]"
          }
        },
        "164746a1fa2144cd996ee9768126baa6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c6d3caab49545af86c6bbb885cba34f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d5f7a2c99bd4461383f8f970c1e2fcd2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "edf7865174914bbc82e10572b91e4282": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f07dd246c274f51a50774eef8bce975": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c97c3158744c4fbcb07e494d9d10ee25": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8f61055f82b143a7a68380d3154c55c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}