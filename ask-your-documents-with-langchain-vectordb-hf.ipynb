{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/peremartramanonellas/ask-your-documents-with-langchain-vectordb-hf?scriptVersionId=137659566\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"from IPython.core.display import HTML\nHTML(\"\"\"\n<style>\n.output_png {\n    display: table-cell;\n    text-align: center;\n    vertical-align: middle;\n    horizontal-align: middle;\n}\nh1 {\n    text-align: center;\n    background-color: #6bacf5;\n    padding: 10px;\n    margin: 0;\n    font-family: monospace;\n    color:DimGray;\n    border-radius: 2px\n    style=\"font-family:verdana;\"\n}\n\nh2 {\n    text-align: center;\n    background-color: #83c2ff;\n    padding: 10px;\n    margin: 0;\n    font-family: monospace;\n    color:DimGray;\n    border-radius: 2px\n}\n\nh3 {\n    text-align: center;\n    background-color: pink;\n    padding: 10px;\n    margin: 0;\n    font-family: monospace;\n    color:DimGray;\n    border-radius: 2px\n}\n\nh4 {\n    text-align: center;\n    background-color: pink;\n    padding: 10px;\n    margin: 0;\n    font-family: monospace;\n    color:DimGray;\n    border-radius: 2px\n}\n\nbody, p {\n    font-family: monospace;\n    font-size: 18px;\n    color: charcoal;\n}\ndiv {\n    font-size: 14px;\n    margin: 0;\n\n}\n</style>\n\"\"\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-07-23T13:56:41.182362Z","iopub.execute_input":"2023-07-23T13:56:41.182989Z","iopub.status.idle":"2023-07-23T13:56:41.195398Z","shell.execute_reply.started":"2023-07-23T13:56:41.182952Z","shell.execute_reply":"2023-07-23T13:56:41.194668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# TUTORIAL HOW TO USE LANGCHAIN AND VECTOR DATABASES TO USE OUR OWN DOCUMENTS WITH HUGGING FACES\n\nIn a [previous notebook](https://www.kaggle.com/code/peremartramanonellas/use-a-vectorial-db-to-optimize-prompts-for-llms), we saw how to use a vectorial database to create an enriched prompt for Hugging Face language models. In this one, we incorporate LangChain so that we can make queries to the language model that take into account our information.\n\nThe information could be our own documents, or whatever was contained in a business knowledge database.\n\nI have prepared the notebook so that it can work with two different Kaggle datasets, so that it is easy to carry out different tests with different Datasets. \n\nThe data has been loaded from a Pandas DataFrame, using the **DataFrameLoader** function from the **document_loaders** library of **LangChain**.\n\n### Feel Free to fork or edit the noteboook for you own convenience. Please consider ***UPVOTING IT***. It helps others to discover the notebook, and it encourages me to continue publishing.","metadata":{}},{"cell_type":"markdown","source":"# Install and load the libraries. \nTo start we need to install the necesary Python packages. \n* **[langchain](https://python.langchain.com/docs/get_started/introduction.html)**. The revolutionary framework to build apps using large language models. \n* **[sentence_transformers](https://www.sbert.net/)**. necesary to create the embeddings we are going to store in the vector database.  \n* **[chromadb](https://www.trychroma.com/)**. This is our vector Database. ChromaDB is easy to use and open source, maybe the most used Vector Database used to store embeddings. ","metadata":{}},{"cell_type":"code","source":"!pip install chromadb\n!pip install langchain\n!pip install sentence_transformers","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-07-23T13:56:46.01745Z","iopub.execute_input":"2023-07-23T13:56:46.018082Z","iopub.status.idle":"2023-07-23T13:58:11.239364Z","shell.execute_reply.started":"2023-07-23T13:56:46.018048Z","shell.execute_reply":"2023-07-23T13:58:11.238254Z"},"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I'm sure that you know the next two packages: Numpy and Pandas, maybe the most used python libraries.\n\nNumpy is a powerful library for numerical computing. \n\nPandas is a library for data manipulation","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd","metadata":{"execution":{"iopub.status.busy":"2023-07-23T13:58:11.24148Z","iopub.execute_input":"2023-07-23T13:58:11.241812Z","iopub.status.idle":"2023-07-23T13:58:11.24813Z","shell.execute_reply.started":"2023-07-23T13:58:11.24178Z","shell.execute_reply":"2023-07-23T13:58:11.246967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load the Dataset\nAs you can see the notebook is ready to work with two different Datasets. Just uncomment the lines of the Dataset you want to use. \n\nAs we are working in a free and limited space, and we can use just 30 gb of memory I limited the number of news to use with the variable MAX_NEWS. If you are using a GPU your memoty will be limited to 16GB. \n\nThe name of the field containing the text of the new is stored in the variable *DOCUMENT* and the metadata in *TOPIC*","metadata":{}},{"cell_type":"code","source":"news = pd.read_csv('/kaggle/input/topic-labeled-news-dataset/labelled_newscatcher_dataset.csv', sep=';')\nMAX_NEWS = 1000\nDOCUMENT=\"title\"\nTOPIC=\"topic\"\n\n#news = pd.read_csv('/kaggle/input/bbc-news/bbc_news.csv')\n#MAX_NEWS = 500\n#DOCUMENT=\"description\"\n#TOPIC=\"title\"\n\n#Because it is just a course we select a small portion of News.\nsubset_news = news.head(MAX_NEWS)","metadata":{"execution":{"iopub.status.busy":"2023-07-23T13:58:11.249666Z","iopub.execute_input":"2023-07-23T13:58:11.250078Z","iopub.status.idle":"2023-07-23T13:58:12.165835Z","shell.execute_reply.started":"2023-07-23T13:58:11.250042Z","shell.execute_reply":"2023-07-23T13:58:12.164666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## CREATE THE DOCUMENT FROM THE DATAFRAME\nWe are going to load the data from a pandas DataFrame. However, LangChain, through the document_loader library, supports multiple data sources, such as Word documents, Excel files, plain text, SQL, and more.\n\nWe also imported the Chroma library, which is used to save the embeddings in the ChromaDB database.","metadata":{}},{"cell_type":"code","source":"from langchain.document_loaders import DataFrameLoader\nfrom langchain.vectorstores import Chroma\n","metadata":{"execution":{"iopub.status.busy":"2023-07-23T13:58:25.702856Z","iopub.execute_input":"2023-07-23T13:58:25.703474Z","iopub.status.idle":"2023-07-23T13:58:28.367913Z","shell.execute_reply.started":"2023-07-23T13:58:25.703426Z","shell.execute_reply":"2023-07-23T13:58:28.366831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"First, we create the loader, indicating the data source and the name of the column in the DataFrame where we store what we could consider as the document, that is, the information we want to pass to the model so that it takes it into account in its responses.","metadata":{}},{"cell_type":"code","source":"df_loader = DataFrameLoader(subset_news, page_content_column=DOCUMENT)","metadata":{"execution":{"iopub.status.busy":"2023-07-23T13:58:28.369955Z","iopub.execute_input":"2023-07-23T13:58:28.370521Z","iopub.status.idle":"2023-07-23T13:58:28.374625Z","shell.execute_reply.started":"2023-07-23T13:58:28.370488Z","shell.execute_reply":"2023-07-23T13:58:28.373872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Then, we use the loader to load the document.","metadata":{}},{"cell_type":"code","source":"df_document = df_loader.load()","metadata":{"execution":{"iopub.status.busy":"2023-07-23T13:58:28.375508Z","iopub.execute_input":"2023-07-23T13:58:28.376473Z","iopub.status.idle":"2023-07-23T13:58:28.487562Z","shell.execute_reply.started":"2023-07-23T13:58:28.376436Z","shell.execute_reply":"2023-07-23T13:58:28.486551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(df_document)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-07-23T13:58:38.679459Z","iopub.execute_input":"2023-07-23T13:58:38.679859Z","iopub.status.idle":"2023-07-23T13:58:38.723068Z","shell.execute_reply.started":"2023-07-23T13:58:38.679827Z","shell.execute_reply":"2023-07-23T13:58:38.722288Z"},"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Creating the embeddings\nFirst, we import a couple of libraries.\n* CharacterTextSplitter: we will use it to group the information contained in different blocks.\n* HuggingFaceEmbeddings: it will create the embeddings in the format that we will store in the database.","metadata":{}},{"cell_type":"code","source":"from langchain.text_splitter import CharacterTextSplitter\n#from langchain.embeddings import HuggingFaceEmbeddings","metadata":{"execution":{"iopub.status.busy":"2023-07-23T14:20:31.424739Z","iopub.execute_input":"2023-07-23T14:20:31.425156Z","iopub.status.idle":"2023-07-23T14:20:31.429477Z","shell.execute_reply.started":"2023-07-23T14:20:31.425127Z","shell.execute_reply":"2023-07-23T14:20:31.428644Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As I said above we split the data into manageable chunks to store as vectors using **CharacterTextSplitter**. There isn't an exact way to do this, more chunks means more detailed context, but will increase the size of our vectorstore.\n\nThere are no magic numbers to inform. It is important to consider that the larger the chunk size, the more context the model will have, but the size of our vector store will also increase.","metadata":{}},{"cell_type":"code","source":"# \ntext_splitter = CharacterTextSplitter(chunk_size=250, chunk_overlap=10)\ntexts = text_splitter.split_documents(df_document)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-07-23T14:20:33.922458Z","iopub.execute_input":"2023-07-23T14:20:33.922852Z","iopub.status.idle":"2023-07-23T14:20:33.953297Z","shell.execute_reply.started":"2023-07-23T14:20:33.92282Z","shell.execute_reply":"2023-07-23T14:20:33.95213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We load the library to create the pre trained model from HuggingFace to create the embeddings from sentences. \n","metadata":{}},{"cell_type":"code","source":"from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\nembedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n\n#embedding_function = HuggingFaceEmbeddings(\n#    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n#)  \n","metadata":{"execution":{"iopub.status.busy":"2023-07-23T14:20:36.499715Z","iopub.execute_input":"2023-07-23T14:20:36.500274Z","iopub.status.idle":"2023-07-23T14:20:55.482587Z","shell.execute_reply.started":"2023-07-23T14:20:36.500231Z","shell.execute_reply":"2023-07-23T14:20:55.481252Z"},"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we are creating the index of embeddings. Using the document, and the embedding function created above. ","metadata":{}},{"cell_type":"code","source":"chromadb_index = Chroma.from_documents(\n    texts, embedding_function, persist_directory='./input'\n)","metadata":{"execution":{"iopub.status.busy":"2023-07-23T14:21:23.191268Z","iopub.execute_input":"2023-07-23T14:21:23.192291Z","iopub.status.idle":"2023-07-23T14:21:43.130126Z","shell.execute_reply.started":"2023-07-23T14:21:23.192247Z","shell.execute_reply":"2023-07-23T14:21:43.128951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## LANGCHAIN\n\nFinally, the time has come to create our chain with LangChain. It will be straightforward. All we do is give it a retriever and a model to call with the result obtained from the retriever.\n\nNow we are going to import RetrievalQA and HuggingFacePipeline classes from langchain module.  ","metadata":{}},{"cell_type":"code","source":"from langchain.chains import RetrievalQA\nfrom langchain.llms import HuggingFacePipeline","metadata":{"execution":{"iopub.status.busy":"2023-07-23T14:21:57.441333Z","iopub.execute_input":"2023-07-23T14:21:57.441765Z","iopub.status.idle":"2023-07-23T14:21:57.447116Z","shell.execute_reply.started":"2023-07-23T14:21:57.44173Z","shell.execute_reply":"2023-07-23T14:21:57.445707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we create the retriever object, the responsible to return the data contained in the ChromaDB Database. ","metadata":{}},{"cell_type":"code","source":"retriever = chromadb_index.as_retriever()","metadata":{"execution":{"iopub.status.busy":"2023-07-23T14:21:58.000055Z","iopub.execute_input":"2023-07-23T14:21:58.00046Z","iopub.status.idle":"2023-07-23T14:21:58.005496Z","shell.execute_reply.started":"2023-07-23T14:21:58.000428Z","shell.execute_reply":"2023-07-23T14:21:58.004278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I tested the notebook with two models from Fugging Face. \n\nThe first one is [dolly-v2-3b](https://huggingface.co/databricks/dolly-v2-3b), the smallest Dolly model. It have 3billion paramaters, more than enough for our sample, and works much better than GPT2. It's a text generation model, and therefore generates slightly more imaginative responses.\n\nThe second one is a t5 model. This is a text2text-generation. so it will produce more concise and succinct responses.\n\nJust be sure the test both, and if you want select other models from Hugging Face. ","metadata":{}},{"cell_type":"code","source":"model_id = \"databricks/dolly-v2-3b\" #my favourite textgeneration model for testing\ntask=\"text-generation\"\n\n#model_id = \"google/flan-t5-large\" #Nice text2text model\n#task=\"text2text-generation\"","metadata":{"execution":{"iopub.status.busy":"2023-07-23T14:52:14.313034Z","iopub.execute_input":"2023-07-23T14:52:14.313707Z","iopub.status.idle":"2023-07-23T14:52:14.319902Z","shell.execute_reply.started":"2023-07-23T14:52:14.313666Z","shell.execute_reply":"2023-07-23T14:52:14.318826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We use HuggingFacePipeline class to create a pipeline for a specific Hugging Face language model. Let's break down the code:\n\n* **model_id**: This is the ID of the Hugging Face language model you want to use. It typically consists of the model name and version.\n* **task**: This parameter specifies the task that you want to perform using the language model. It could be \"text-generation\", \"text2text-generation\", \"question-answering\", or other tasks supported by the model.\n* **model_kwargs**: Allows you to provide additional arguments specific to the chosen model. In this case, it sets \"temperature\" to 0 (indicating deterministic output) and \"max_length\" to 256, which limits the maximum length of generated text to 256 tokens.\n","metadata":{}},{"cell_type":"code","source":"hf_llm = HuggingFacePipeline.from_model_id(\n    model_id=model_id,\n    task=task,\n    model_kwargs={\n        \"temperature\": 0,\n        \"max_length\": 256\n    },\n)","metadata":{"execution":{"iopub.status.busy":"2023-07-23T14:52:16.375219Z","iopub.execute_input":"2023-07-23T14:52:16.375681Z","iopub.status.idle":"2023-07-23T14:53:21.628179Z","shell.execute_reply.started":"2023-07-23T14:52:16.375645Z","shell.execute_reply":"2023-07-23T14:53:21.627127Z"},"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We are setting up the ***document_qa***, a **RetrievalQA** object, that we are going to use to run the questions. \n\nThe ***stuff*** type is the simplest type of chain that we can have. I get the documents from the retiever and use the language model to obtain responses. ","metadata":{}},{"cell_type":"code","source":"chain_type = \"stuff\"  \ndocument_qa = RetrievalQA.from_chain_type(\n    llm=hf_llm, chain_type=\"stuff\", retriever=retriever\n)","metadata":{"execution":{"iopub.status.busy":"2023-07-23T14:53:21.630601Z","iopub.execute_input":"2023-07-23T14:53:21.631493Z","iopub.status.idle":"2023-07-23T14:53:22.438569Z","shell.execute_reply.started":"2023-07-23T14:53:21.631443Z","shell.execute_reply":"2023-07-23T14:53:22.437601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Time to call the chain and obtain the responses!","metadata":{}},{"cell_type":"code","source":"#Sample question for newscatcher dataset. \nresponse = document_qa.run(\"Can I buy a Toshiba laptop?\")\n\n#Sample question for BBC Dataset. \n#response = document_qa.run(\"Who is going to meet boris johnson?\")\ndisplay(response)","metadata":{"execution":{"iopub.status.busy":"2023-07-23T14:53:22.44028Z","iopub.execute_input":"2023-07-23T14:53:22.440752Z","iopub.status.idle":"2023-07-23T14:53:23.609547Z","shell.execute_reply.started":"2023-07-23T14:53:22.440712Z","shell.execute_reply":"2023-07-23T14:53:23.608302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusions, Fork and Improve\nA very short notebook, but with a lot of content.\n\nWe have used a vectorial database to store the information from a Kaggle dataset. We have incorporated it into a LangChain chain through a retriever, and now we are able to make queries about the information contained in the dataset to a couple of Hugging Face Language Models.\n\nPlease don't stop here.\n\n* The notebook is prepared to use two Datasets. Do tests with it.An if you have time select other Datset from Kaggle. \n\n* Find another model on Hugging Face and compare it.\n\n* We loade the data from a Dataframe, try to upload Data from a .txt file. \n\n## Continue learning\nThis notebook is part of a [course on large language models](https://github.com/peremartra/Large-Language-Model-Notebooks-Course) I'm working on and it's available on [GitHub](https://github.com/peremartra/Large-Language-Model-Notebooks-Course). You can see the other lessons and if you like it, don't forget to subscribe to receive notifications of new lessons.\n\n","metadata":{"execution":{"iopub.status.busy":"2023-07-12T22:01:56.992775Z","iopub.execute_input":"2023-07-12T22:01:56.993351Z","iopub.status.idle":"2023-07-12T22:01:57.001309Z","shell.execute_reply.started":"2023-07-12T22:01:56.993305Z","shell.execute_reply":"2023-07-12T22:01:56.999431Z"}}},{"cell_type":"markdown","source":"### If you liked the notebook Please consider ***UPVOTING IT***. It helps others to discover it, and encourages me to continue publishing.","metadata":{"execution":{"iopub.status.busy":"2023-07-12T22:17:34.63605Z","iopub.execute_input":"2023-07-12T22:17:34.636604Z","iopub.status.idle":"2023-07-12T22:17:34.644497Z","shell.execute_reply.started":"2023-07-12T22:17:34.636566Z","shell.execute_reply":"2023-07-12T22:17:34.642819Z"}}}]}