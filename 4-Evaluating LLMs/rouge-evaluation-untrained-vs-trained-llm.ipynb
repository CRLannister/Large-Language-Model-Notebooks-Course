{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/peremartramanonellas/rouge-evaluation-untrained-vs-trained-llm?scriptVersionId=142768858\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"from IPython.core.display import HTML\nHTML(\"\"\"\n<style>\n.output_png {\n    display: table-cell;\n    text-align: center;\n    vertical-align: middle;\n    horizontal-align: middle;\n}\nh1 {\n    text-align: center;\n    background-color: #6bacf5;\n    padding: 10px;\n    margin: 0;\n    font-family: monospace;\n    color:DimGray;\n    border-radius: 2px\n    style=\"font-family:verdana;\"\n}\n\nh2 {\n    text-align: center;\n    background-color: #83c2ff;\n    padding: 10px;\n    margin: 0;\n    font-family: monospace;\n    color:DimGray;\n    border-radius: 2px\n}\n\nh3 {\n    text-align: center;\n    background-color: pink;\n    padding: 10px;\n    margin: 0;\n    font-family: monospace;\n    color:DimGray;\n    border-radius: 2px\n}\n\nh4 {\n    text-align: center;\n    background-color: pink;\n    padding: 10px;\n    margin: 0;\n    font-family: monospace;\n    color:DimGray;\n    border-radius: 2px\n}\n\nbody, p {\n    font-family: monospace;\n    font-size: 18px;\n    color: charcoal;\n}\ndiv {\n    font-size: 14px;\n    margin: 0;\n\n}\n\n\n</style>\n\"\"\")","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-09-12T15:04:30.237035Z","iopub.execute_input":"2023-09-12T15:04:30.237903Z","iopub.status.idle":"2023-09-12T15:04:30.277082Z","shell.execute_reply.started":"2023-09-12T15:04:30.237855Z","shell.execute_reply":"2023-09-12T15:04:30.275908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# How to Evaluate Large Language Models for Summarization Using ROUGE.\nThe way we evaluate large language models is quite different from evaluating machine learning models, where metrics like Accuracy, F1 Score, or Recall were commonly used. \n\nMetrics for generated language are distinct. Depending on the specific application, different metrics are chosen to assess the model's performance. \n\nIn this notebook, we will explore the usage of the ROUGE metric to measure the quality of summaries generated by a language model.\n\n### This notebook is part of a comprehensive course on Large Language Models available on GitHub: https://github.com/peremartra/Large-Language-Model-Notebooks-Course. If you want to stay informed about new lessons or updates, simply follow or star the repository.\n\n## What is ROUGE?\nROUGE isn't just a single metric; it's a set of metrics that measure the overlap and similarity between the generated summary and a reference summary that serves as a benchmark.\n\nIt returns fourth individual metrics. The metrics provided are:\n\n* ROUGE-1: Measures the overlap of unigrams, or single words.\n* ROUGE-2: Measures the overlap of bigrams, or pairs of words.\n* ROUGE-L: Measures the longest common subsequence, rewarding longer shared sequences between the generated and reference summaries.\n* ROUGE-LSUM: Calculated as the length of the LCS divided by the sum of the lengths of the generated summary and the reference summary. \n\n### Feel Free to fork or edit the noteboook for you own convenience. Please consider ***UPVOTING IT***. It helps others to discover the notebook, and it encourages me to continue publishing.\n\n## What are we going to do? \nWe are going to use two T5 models, one of them being the t5-Base model and the other a t5-base fine-tuned  specifically designed for creating summaries. \n\nFirst, we will use a dataset and generate summaries using both models. By comparing the two generated summaries, we can observe whether the fine-tuning has been effective in producing different results. In other words, here we will only determine that the two models exhibit significant differences in summary generation, but we won't know which one might perform better.\n\nTo determine which model generates better summaries, we will utilize a well-known dataset called 'cnn_dailymail,' which is available in the 'datasets' library. \n\nThis dataset contains reference summaries that can be used for comparison. We will assess the summaries generated by the two models against these reference summaries.\n\nThe model that obtains a higher ROUGE score will be considered the one that produces better summaries.\n\n## The models.\nt5-Base Finnetunned: https://huggingface.co/flax-community/t5-base-cnn-\n\nt5-Base: https://huggingface.co/t5-base\n","metadata":{}},{"cell_type":"markdown","source":"# Load the Data","metadata":{}},{"cell_type":"code","source":"#Import generic libraries\nimport numpy as np \nimport pandas as pd\nimport torch\n","metadata":{"execution":{"iopub.status.busy":"2023-09-12T15:04:30.278817Z","iopub.execute_input":"2023-09-12T15:04:30.279173Z","iopub.status.idle":"2023-09-12T15:04:34.335706Z","shell.execute_reply.started":"2023-09-12T15:04:30.279143Z","shell.execute_reply":"2023-09-12T15:04:34.334786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The dataset is available on Kaggle and comprises a collection of technological news articles compiled by MIT. The article text is located in the 'Article Body' column.\n\nhttps://www.kaggle.com/datasets/deepanshudalal09/mit-ai-news-published-till-2023","metadata":{}},{"cell_type":"code","source":"news = pd.read_csv('/kaggle/input/mit-ai-news-published-till-2023/articles.csv')\nDOCUMENT=\"Article Body\"","metadata":{"execution":{"iopub.status.busy":"2023-09-12T15:04:34.337278Z","iopub.execute_input":"2023-09-12T15:04:34.338142Z","iopub.status.idle":"2023-09-12T15:04:34.524683Z","shell.execute_reply.started":"2023-09-12T15:04:34.338093Z","shell.execute_reply":"2023-09-12T15:04:34.523559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Because it is just a course we select a small portion of News.\nMAX_NEWS = 3\nsubset_news = news.head(MAX_NEWS)","metadata":{"execution":{"iopub.status.busy":"2023-09-12T15:04:34.527339Z","iopub.execute_input":"2023-09-12T15:04:34.527679Z","iopub.status.idle":"2023-09-12T15:04:34.533419Z","shell.execute_reply.started":"2023-09-12T15:04:34.527651Z","shell.execute_reply":"2023-09-12T15:04:34.531958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"subset_news.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-12T15:04:34.534766Z","iopub.execute_input":"2023-09-12T15:04:34.535371Z","iopub.status.idle":"2023-09-12T15:04:34.573905Z","shell.execute_reply.started":"2023-09-12T15:04:34.535339Z","shell.execute_reply":"2023-09-12T15:04:34.572731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"articles = subset_news[DOCUMENT].tolist()","metadata":{"execution":{"iopub.status.busy":"2023-09-12T15:04:34.575511Z","iopub.execute_input":"2023-09-12T15:04:34.576597Z","iopub.status.idle":"2023-09-12T15:04:34.582401Z","shell.execute_reply.started":"2023-09-12T15:04:34.576555Z","shell.execute_reply":"2023-09-12T15:04:34.580874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load the Models and create the summaries\n\nBoth models are available on Hugging Face, so we will work with the Transformers library.","metadata":{}},{"cell_type":"code","source":"import transformers\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\nmodel_name_small = \"t5-base\"\nmodel_name_reference = \"flax-community/t5-base-cnn-dm\"\n#model_name_reference = \"pszemraj/long-t5-tglobal-base-16384-booksum-V11-big_patent-V2\"","metadata":{"execution":{"iopub.status.busy":"2023-09-12T15:04:34.584423Z","iopub.execute_input":"2023-09-12T15:04:34.585276Z","iopub.status.idle":"2023-09-12T15:04:36.957992Z","shell.execute_reply.started":"2023-09-12T15:04:34.585229Z","shell.execute_reply":"2023-09-12T15:04:36.956734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#This function returns the tokenizer and the Model. \ndef get_model(model_id):\n    tokenizer = AutoTokenizer.from_pretrained(model_id)\n    model = AutoModelForSeq2SeqLM.from_pretrained(model_id)\n    \n    return tokenizer, model\n    ","metadata":{"execution":{"iopub.status.busy":"2023-09-12T15:04:36.959556Z","iopub.execute_input":"2023-09-12T15:04:36.960146Z","iopub.status.idle":"2023-09-12T15:04:36.966175Z","shell.execute_reply.started":"2023-09-12T15:04:36.960092Z","shell.execute_reply":"2023-09-12T15:04:36.965211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer_small, model_small = get_model(model_name_small)","metadata":{"execution":{"iopub.status.busy":"2023-09-12T15:04:36.967604Z","iopub.execute_input":"2023-09-12T15:04:36.967971Z","iopub.status.idle":"2023-09-12T15:05:06.09361Z","shell.execute_reply.started":"2023-09-12T15:04:36.967929Z","shell.execute_reply":"2023-09-12T15:05:06.092225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer_reference, model_reference = get_model(model_name_reference)","metadata":{"execution":{"iopub.status.busy":"2023-09-12T15:05:06.09811Z","iopub.execute_input":"2023-09-12T15:05:06.098525Z","iopub.status.idle":"2023-09-12T15:05:59.989243Z","shell.execute_reply.started":"2023-09-12T15:05:06.098486Z","shell.execute_reply":"2023-09-12T15:05:59.988213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"With both models downloaded and ready, we create a function that will perform the summaries.\n\nThe function takes fourth parameters:\n\n* the list of texts to summarize.\n* the tokenizer.\n* the model.\n* the maximum length for the generated summary","metadata":{}},{"cell_type":"code","source":"def create_summaries(texts_list, tokenizer, model, max_l=125):\n    \n    # We are going to add a prefix to each article to be summarized \n    # so that the model knows what it should do\n    prefix = \"Summarize this news: \"  \n    summaries_list = [] #Will contain all summaries\n\n    texts_list = [prefix + text for text in texts_list]\n    \n    for text in texts_list:\n        \n        summary=\"\"\n        \n        #calculate the encodings\n        input_encodings = tokenizer(text, \n                                    max_length=1024, \n                                    return_tensors='pt', \n                                    padding=True, \n                                    truncation=True)\n\n        # Generate summaries\n        with torch.no_grad():\n            output = model.generate(\n                input_ids=input_encodings.input_ids,\n                attention_mask=input_encodings.attention_mask,\n                max_length=max_l,  # Set the maximum length of the generated summary\n                num_beams=2,     # Set the number of beams for beam search\n                early_stopping=True\n            )\n            \n        #Decode to get the text\n        summary = tokenizer.batch_decode(output, skip_special_tokens=True)\n        \n        #Add the summary to summaries list \n        summaries_list += summary\n    return summaries_list \n    ","metadata":{"execution":{"iopub.status.busy":"2023-09-12T15:05:59.99094Z","iopub.execute_input":"2023-09-12T15:05:59.991764Z","iopub.status.idle":"2023-09-12T15:06:00.001952Z","shell.execute_reply.started":"2023-09-12T15:05:59.99172Z","shell.execute_reply":"2023-09-12T15:06:00.000704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To create the summaries, we call the 'create_summaries' function, passing both the news articles and the corresponding tokenizer and model.","metadata":{}},{"cell_type":"code","source":"# Creating the summaries for both models. \nsummaries_small = create_summaries(articles, \n                                  tokenizer_small, \n                                  model_small)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-12T15:06:00.003767Z","iopub.execute_input":"2023-09-12T15:06:00.004184Z","iopub.status.idle":"2023-09-12T15:06:31.841199Z","shell.execute_reply.started":"2023-09-12T15:06:00.004079Z","shell.execute_reply":"2023-09-12T15:06:31.839433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"summaries_reference = create_summaries(articles, \n                                      tokenizer_reference, \n                                      model_reference)","metadata":{"execution":{"iopub.status.busy":"2023-09-12T15:06:31.843276Z","iopub.execute_input":"2023-09-12T15:06:31.843742Z","iopub.status.idle":"2023-09-12T15:07:05.052825Z","shell.execute_reply.started":"2023-09-12T15:06:31.843702Z","shell.execute_reply":"2023-09-12T15:07:05.051575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"summaries_small","metadata":{"execution":{"iopub.status.busy":"2023-09-12T15:07:05.054786Z","iopub.execute_input":"2023-09-12T15:07:05.055486Z","iopub.status.idle":"2023-09-12T15:07:05.063428Z","shell.execute_reply.started":"2023-09-12T15:07:05.055435Z","shell.execute_reply":"2023-09-12T15:07:05.062089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"summaries_reference","metadata":{"execution":{"iopub.status.busy":"2023-09-12T15:07:05.064883Z","iopub.execute_input":"2023-09-12T15:07:05.065645Z","iopub.status.idle":"2023-09-12T15:07:05.078376Z","shell.execute_reply.started":"2023-09-12T15:07:05.065609Z","shell.execute_reply":"2023-09-12T15:07:05.077574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"At first glance, it's evident that the summaries are different. \n\nHowever, it's challenging to determine which one is better. \n\nIt's even difficult to discern whether they are significantly distinct or if there are just subtle differences between them.\n\nThis is what we are going to verify now using ROUGE. When comparing the summaries of one model with those of the other, we don't get an idea of which one is better, but rather an idea of how much the summaries have changed with the fine-tuning applied to the model.","metadata":{}},{"cell_type":"markdown","source":"# ROUGE\nLet's install and load all the necessary libraries to conduct a ROUGE evaluation.","metadata":{}},{"cell_type":"code","source":"!pip install evaluate\n!pip install rouge_score","metadata":{"execution":{"iopub.status.busy":"2023-09-12T15:11:28.806347Z","iopub.execute_input":"2023-09-12T15:11:28.806753Z","iopub.status.idle":"2023-09-12T15:11:58.966857Z","shell.execute_reply.started":"2023-09-12T15:11:28.806724Z","shell.execute_reply":"2023-09-12T15:11:58.965563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import evaluate\nfrom nltk.tokenize import sent_tokenize\n\n#!pip install rouge","metadata":{"execution":{"iopub.status.busy":"2023-09-12T15:11:58.969437Z","iopub.execute_input":"2023-09-12T15:11:58.969817Z","iopub.status.idle":"2023-09-12T15:11:58.975447Z","shell.execute_reply.started":"2023-09-12T15:11:58.969779Z","shell.execute_reply":"2023-09-12T15:11:58.974181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#import evaluate\n#from nltk.tokenize import sent_tokenize\n#from rouge_score import rouge_scorer","metadata":{"execution":{"iopub.status.busy":"2023-09-12T15:07:05.991486Z","iopub.status.idle":"2023-09-12T15:07:05.991893Z","shell.execute_reply.started":"2023-09-12T15:07:05.991695Z","shell.execute_reply":"2023-09-12T15:07:05.991714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#With the function load of the library evaluate \n#we create a rouge_score object\nrouge_score = evaluate.load(\"rouge\")","metadata":{"execution":{"iopub.status.busy":"2023-09-12T15:12:30.861518Z","iopub.execute_input":"2023-09-12T15:12:30.861983Z","iopub.status.idle":"2023-09-12T15:12:32.323908Z","shell.execute_reply.started":"2023-09-12T15:12:30.861949Z","shell.execute_reply":"2023-09-12T15:12:32.322793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Calculating ROUGE is as simple as calling the *compute* function of the *rouge_score* object we created earlier. This function takes the texts to compare as arguments and a third value *use_stemmer*, which indicates whether it should use *stemmer* or full words for the comparison.\n\nA *stemmer* is the base of the word. Transform differents forms of a word in a same base. \n\nSome samples of steammer are: \n* Jumping -> Jump. \n* Running -> Run. \n* Cats -> Cat. ","metadata":{}},{"cell_type":"code","source":"def compute_rouge_score(generated, reference):\n    \n    #We need to add '\\n' to each line before send it to ROUGE\n    generated_with_newlines = [\"\\n\".join(sent_tokenize(s.strip())) for s in generated]\n    reference_with_newlines = [\"\\n\".join(sent_tokenize(s.strip())) for s in reference]\n    \n    return rouge_score.compute(\n        predictions=generated_with_newlines,\n        references=reference_with_newlines,\n        use_stemmer=True,\n        \n    )","metadata":{"execution":{"iopub.status.busy":"2023-09-12T15:12:36.527065Z","iopub.execute_input":"2023-09-12T15:12:36.52748Z","iopub.status.idle":"2023-09-12T15:12:36.534808Z","shell.execute_reply.started":"2023-09-12T15:12:36.527449Z","shell.execute_reply":"2023-09-12T15:12:36.533627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"compute_rouge_score(summaries_small, summaries_reference)","metadata":{"execution":{"iopub.status.busy":"2023-09-12T15:12:40.646712Z","iopub.execute_input":"2023-09-12T15:12:40.647148Z","iopub.status.idle":"2023-09-12T15:12:41.011346Z","shell.execute_reply.started":"2023-09-12T15:12:40.6471Z","shell.execute_reply":"2023-09-12T15:12:41.009739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that there is a difference between the two models when performing summarization. \n\nFor example, in ROUGE-1, the similarity is 47%, while in ROUGE-2, it's a 32%. This indicates that the results are different, with some similarities but differents enough. \n\nHowever, we still don't know which model is better since we have compared them to each other and not to a reference text. But at the very least, we know that the fine-tuning process applied to the second model has significantly altered its results.","metadata":{}},{"cell_type":"markdown","source":"# Comparing to a Dataset with real summaries. \nWe are going to load the Dataset cnn_dailymail. This is a well-known dataset available in the **Datasets** library, and it suits our purpose perfectly. \n\nApart from the news, it also contains pre-existing summaries. \n\nWe will compare the summaries generated by the two models we are using with those from the dataset to determine which model creates summaries that are closer to the reference ones.","metadata":{"execution":{"iopub.status.busy":"2023-08-07T17:21:28.296173Z","iopub.execute_input":"2023-08-07T17:21:28.296636Z","iopub.status.idle":"2023-08-07T17:21:28.301906Z","shell.execute_reply.started":"2023-08-07T17:21:28.296601Z","shell.execute_reply":"2023-08-07T17:21:28.300702Z"}}},{"cell_type":"code","source":"from datasets import load_dataset\n\ncnn_dataset = load_dataset(\n    \"cnn_dailymail\", version=\"3.0.0\"\n)\n\n#Get just a few news to test\nsample_cnn = cnn_dataset[\"test\"].select(range(MAX_NEWS))\n\nsample_cnn","metadata":{"execution":{"iopub.status.busy":"2023-09-12T15:12:45.79725Z","iopub.execute_input":"2023-09-12T15:12:45.797677Z","iopub.status.idle":"2023-09-12T15:16:53.672497Z","shell.execute_reply.started":"2023-09-12T15:12:45.79764Z","shell.execute_reply":"2023-09-12T15:16:53.671301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We retrieve the maximum length of the summaries to give the models the option to generate summaries of the same length, if they choose to do so.","metadata":{}},{"cell_type":"code","source":"max_length = max(len(item['highlights']) for item in sample_cnn)\nmax_length = max_length + 10","metadata":{"execution":{"iopub.status.busy":"2023-09-12T15:17:02.366072Z","iopub.execute_input":"2023-09-12T15:17:02.366563Z","iopub.status.idle":"2023-09-12T15:17:02.37431Z","shell.execute_reply.started":"2023-09-12T15:17:02.366523Z","shell.execute_reply":"2023-09-12T15:17:02.373015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"summaries_t5_base = create_summaries(sample_cnn[\"article\"], \n                                      tokenizer_small, \n                                      model_small, \n                                      max_l=max_length)","metadata":{"execution":{"iopub.status.busy":"2023-09-12T15:17:03.562568Z","iopub.execute_input":"2023-09-12T15:17:03.563215Z","iopub.status.idle":"2023-09-12T15:17:26.820027Z","shell.execute_reply.started":"2023-09-12T15:17:03.56318Z","shell.execute_reply":"2023-09-12T15:17:26.818903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"summaries_t5_finetuned = create_summaries(sample_cnn[\"article\"], \n                                      tokenizer_reference, \n                                      model_reference, \n                                      max_l=max_length)","metadata":{"execution":{"iopub.status.busy":"2023-09-12T15:18:49.988417Z","iopub.execute_input":"2023-09-12T15:18:49.98889Z","iopub.status.idle":"2023-09-12T15:19:11.266176Z","shell.execute_reply.started":"2023-09-12T15:18:49.988854Z","shell.execute_reply":"2023-09-12T15:19:11.264767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Get the real summaries from the cnn_dataset\nreal_summaries = sample_cnn['highlights']","metadata":{"execution":{"iopub.status.busy":"2023-09-12T15:19:37.145324Z","iopub.execute_input":"2023-09-12T15:19:37.145765Z","iopub.status.idle":"2023-09-12T15:19:37.152062Z","shell.execute_reply.started":"2023-09-12T15:19:37.145733Z","shell.execute_reply":"2023-09-12T15:19:37.150764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's take a look at the generated summaries alongside the reference summaries provided by the dataset.","metadata":{}},{"cell_type":"code","source":"summaries = pd.DataFrame.from_dict(\n        {\n            \"base\": summaries_t5_base, \n            \"finetuned\": summaries_t5_finetuned,\n            \"reference\": real_summaries,\n        }\n    )\nsummaries.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-12T15:19:40.579644Z","iopub.execute_input":"2023-09-12T15:19:40.580043Z","iopub.status.idle":"2023-09-12T15:19:40.595382Z","shell.execute_reply.started":"2023-09-12T15:19:40.580013Z","shell.execute_reply":"2023-09-12T15:19:40.594207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we can calculate the ROUGE scores for the two models.","metadata":{}},{"cell_type":"code","source":"compute_rouge_score(summaries_t5_base, real_summaries)","metadata":{"execution":{"iopub.status.busy":"2023-09-12T15:19:43.333406Z","iopub.execute_input":"2023-09-12T15:19:43.333808Z","iopub.status.idle":"2023-09-12T15:19:43.591965Z","shell.execute_reply.started":"2023-09-12T15:19:43.333778Z","shell.execute_reply":"2023-09-12T15:19:43.590757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"compute_rouge_score(summaries_t5_finetuned, real_summaries)","metadata":{"execution":{"iopub.status.busy":"2023-09-12T15:19:45.558785Z","iopub.execute_input":"2023-09-12T15:19:45.55926Z","iopub.status.idle":"2023-09-12T15:19:45.80719Z","shell.execute_reply.started":"2023-09-12T15:19:45.559222Z","shell.execute_reply":"2023-09-12T15:19:45.805976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"With these results, I would say that the fine-tuned model performs slightly better than the T5-Base model. It consistently achieves higher ROUGE scores in all metrics except for LSUM, where the difference is minimal.\n\nAdditionally, the ROUGE metrics are quite interpretable. \n\nLSUM indicates the percentage of the longest common subsequence, regardless of word order, in relation to the total length of the text. \n\nThis can be a good indicator of overall similarity between texts. However, both models have very similar LSUM scores, and the fine-tuned model has better scores in other ROUGE metrics.\n\nPersonally, I would lean towards the fine-tuned model, although the difference may not be very significant.\n\n## Continue learning\nThis notebook is part of a [course on large language models](https://github.com/peremartra/Large-Language-Model-Notebooks-Course) I'm working on and it's available on [GitHub](https://github.com/peremartra/Large-Language-Model-Notebooks-Course). You can see the other lessons and if you like it, don't forget to subscribe to receive notifications of new lessons.\n\nOther notebooks in the Large Language Models series: \nhttps://www.kaggle.com/code/peremartramanonellas/use-a-vectorial-db-to-optimize-prompts-for-llms\nhttps://www.kaggle.com/code/peremartramanonellas/ask-your-documents-with-langchain-vectordb-hf\n","metadata":{}},{"cell_type":"markdown","source":"### Feel Free to fork or edit the noteboook for you own convenience. Please consider ***UPVOTING IT***. It helps others to discover the notebook, and it encourages me to continue publishing.","metadata":{}}],"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}}