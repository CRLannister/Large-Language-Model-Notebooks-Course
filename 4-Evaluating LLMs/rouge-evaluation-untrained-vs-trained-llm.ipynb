{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.kaggle.com/code/peremartramanonellas/rouge-evaluation-untrained-vs-trained-llm?scriptVersionId=142771422\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2023-09-12T15:04:30.237903Z",
     "iopub.status.busy": "2023-09-12T15:04:30.237035Z",
     "iopub.status.idle": "2023-09-12T15:04:30.277082Z",
     "shell.execute_reply": "2023-09-12T15:04:30.275908Z",
     "shell.execute_reply.started": "2023-09-12T15:04:30.237855Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(\"\"\"\n",
    "<style>\n",
    ".output_png {\n",
    "    display: table-cell;\n",
    "    text-align: center;\n",
    "    vertical-align: middle;\n",
    "    horizontal-align: middle;\n",
    "}\n",
    "h1 {\n",
    "    text-align: center;\n",
    "    background-color: #6bacf5;\n",
    "    padding: 10px;\n",
    "    margin: 0;\n",
    "    font-family: monospace;\n",
    "    color:DimGray;\n",
    "    border-radius: 2px\n",
    "    style=\"font-family:verdana;\"\n",
    "}\n",
    "\n",
    "h2 {\n",
    "    text-align: center;\n",
    "    background-color: #83c2ff;\n",
    "    padding: 10px;\n",
    "    margin: 0;\n",
    "    font-family: monospace;\n",
    "    color:DimGray;\n",
    "    border-radius: 2px\n",
    "}\n",
    "\n",
    "h3 {\n",
    "    text-align: center;\n",
    "    background-color: pink;\n",
    "    padding: 10px;\n",
    "    margin: 0;\n",
    "    font-family: monospace;\n",
    "    color:DimGray;\n",
    "    border-radius: 2px\n",
    "}\n",
    "\n",
    "h4 {\n",
    "    text-align: center;\n",
    "    background-color: pink;\n",
    "    padding: 10px;\n",
    "    margin: 0;\n",
    "    font-family: monospace;\n",
    "    color:DimGray;\n",
    "    border-radius: 2px\n",
    "}\n",
    "\n",
    "body, p {\n",
    "    font-family: monospace;\n",
    "    font-size: 18px;\n",
    "    color: charcoal;\n",
    "}\n",
    "div {\n",
    "    font-size: 14px;\n",
    "    margin: 0;\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "</style>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Evaluate Large Language Models for Summarization Using ROUGE.\n",
    "The way we evaluate large language models is quite different from evaluating machine learning models, where metrics like Accuracy, F1 Score, or Recall were commonly used. \n",
    "\n",
    "Metrics for generated language are distinct. Depending on the specific application, different metrics are chosen to assess the model's performance. \n",
    "\n",
    "In this notebook, we will explore the usage of the ROUGE metric to measure the quality of summaries generated by a language model.\n",
    "\n",
    "### This notebook is part of a comprehensive course on Large Language Models available on GitHub: https://github.com/peremartra/Large-Language-Model-Notebooks-Course. If you want to stay informed about new lessons or updates, simply follow or star the repository.\n",
    "\n",
    "## What is ROUGE?\n",
    "ROUGE isn't just a single metric; it's a set of metrics that measure the overlap and similarity between the generated summary and a reference summary that serves as a benchmark.\n",
    "\n",
    "It returns fourth individual metrics. The metrics provided are:\n",
    "\n",
    "* ROUGE-1: Measures the overlap of unigrams, or single words.\n",
    "* ROUGE-2: Measures the overlap of bigrams, or pairs of words.\n",
    "* ROUGE-L: Measures the longest common subsequence, rewarding longer shared sequences between the generated and reference summaries.\n",
    "* ROUGE-LSUM: Calculated as the length of the LCS divided by the sum of the lengths of the generated summary and the reference summary. \n",
    "\n",
    "### Feel Free to fork or edit the noteboook for you own convenience. Please consider ***UPVOTING IT***. It helps others to discover the notebook, and it encourages me to continue publishing.\n",
    "\n",
    "## What are we going to do? \n",
    "We are going to use two T5 models, one of them being the t5-Base model and the other a t5-base fine-tuned  specifically designed for creating summaries. \n",
    "\n",
    "First, we will use a dataset and generate summaries using both models. By comparing the two generated summaries, we can observe whether the fine-tuning has been effective in producing different results. In other words, here we will only determine that the two models exhibit significant differences in summary generation, but we won't know which one might perform better.\n",
    "\n",
    "To determine which model generates better summaries, we will utilize a well-known dataset called 'cnn_dailymail,' which is available in the 'datasets' library. \n",
    "\n",
    "This dataset contains reference summaries that can be used for comparison. We will assess the summaries generated by the two models against these reference summaries.\n",
    "\n",
    "The model that obtains a higher ROUGE score will be considered the one that produces better summaries.\n",
    "\n",
    "## The models.\n",
    "t5-Base Finnetunned: https://huggingface.co/flax-community/t5-base-cnn-\n",
    "\n",
    "t5-Base: https://huggingface.co/t5-base\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-12T15:04:30.279173Z",
     "iopub.status.busy": "2023-09-12T15:04:30.278817Z",
     "iopub.status.idle": "2023-09-12T15:04:34.335706Z",
     "shell.execute_reply": "2023-09-12T15:04:34.334786Z",
     "shell.execute_reply.started": "2023-09-12T15:04:30.279143Z"
    }
   },
   "outputs": [],
   "source": [
    "#Import generic libraries\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is available on Kaggle and comprises a collection of technological news articles compiled by MIT. The article text is located in the 'Article Body' column.\n",
    "\n",
    "https://www.kaggle.com/datasets/deepanshudalal09/mit-ai-news-published-till-2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-12T15:04:34.338142Z",
     "iopub.status.busy": "2023-09-12T15:04:34.337278Z",
     "iopub.status.idle": "2023-09-12T15:04:34.524683Z",
     "shell.execute_reply": "2023-09-12T15:04:34.523559Z",
     "shell.execute_reply.started": "2023-09-12T15:04:34.338093Z"
    }
   },
   "outputs": [],
   "source": [
    "news = pd.read_csv('/kaggle/input/mit-ai-news-published-till-2023/articles.csv')\n",
    "DOCUMENT=\"Article Body\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-12T15:04:34.527679Z",
     "iopub.status.busy": "2023-09-12T15:04:34.527339Z",
     "iopub.status.idle": "2023-09-12T15:04:34.533419Z",
     "shell.execute_reply": "2023-09-12T15:04:34.531958Z",
     "shell.execute_reply.started": "2023-09-12T15:04:34.527651Z"
    }
   },
   "outputs": [],
   "source": [
    "#Because it is just a course we select a small portion of News.\n",
    "MAX_NEWS = 3\n",
    "subset_news = news.head(MAX_NEWS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-12T15:04:34.535371Z",
     "iopub.status.busy": "2023-09-12T15:04:34.534766Z",
     "iopub.status.idle": "2023-09-12T15:04:34.573905Z",
     "shell.execute_reply": "2023-09-12T15:04:34.572731Z",
     "shell.execute_reply.started": "2023-09-12T15:04:34.535339Z"
    }
   },
   "outputs": [],
   "source": [
    "subset_news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-12T15:04:34.576597Z",
     "iopub.status.busy": "2023-09-12T15:04:34.575511Z",
     "iopub.status.idle": "2023-09-12T15:04:34.582401Z",
     "shell.execute_reply": "2023-09-12T15:04:34.580874Z",
     "shell.execute_reply.started": "2023-09-12T15:04:34.576555Z"
    }
   },
   "outputs": [],
   "source": [
    "articles = subset_news[DOCUMENT].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Models and create the summaries\n",
    "\n",
    "Both models are available on Hugging Face, so we will work with the Transformers library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-12T15:04:34.585276Z",
     "iopub.status.busy": "2023-09-12T15:04:34.584423Z",
     "iopub.status.idle": "2023-09-12T15:04:36.957992Z",
     "shell.execute_reply": "2023-09-12T15:04:36.956734Z",
     "shell.execute_reply.started": "2023-09-12T15:04:34.585229Z"
    }
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "model_name_small = \"t5-base\"\n",
    "model_name_reference = \"flax-community/t5-base-cnn-dm\"\n",
    "#model_name_reference = \"pszemraj/long-t5-tglobal-base-16384-booksum-V11-big_patent-V2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-12T15:04:36.960146Z",
     "iopub.status.busy": "2023-09-12T15:04:36.959556Z",
     "iopub.status.idle": "2023-09-12T15:04:36.966175Z",
     "shell.execute_reply": "2023-09-12T15:04:36.965211Z",
     "shell.execute_reply.started": "2023-09-12T15:04:36.960092Z"
    }
   },
   "outputs": [],
   "source": [
    "#This function returns the tokenizer and the Model. \n",
    "def get_model(model_id):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_id)\n",
    "    \n",
    "    return tokenizer, model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-12T15:04:36.967971Z",
     "iopub.status.busy": "2023-09-12T15:04:36.967604Z",
     "iopub.status.idle": "2023-09-12T15:05:06.09361Z",
     "shell.execute_reply": "2023-09-12T15:05:06.092225Z",
     "shell.execute_reply.started": "2023-09-12T15:04:36.967929Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer_small, model_small = get_model(model_name_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-12T15:05:06.098525Z",
     "iopub.status.busy": "2023-09-12T15:05:06.09811Z",
     "iopub.status.idle": "2023-09-12T15:05:59.989243Z",
     "shell.execute_reply": "2023-09-12T15:05:59.988213Z",
     "shell.execute_reply.started": "2023-09-12T15:05:06.098486Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer_reference, model_reference = get_model(model_name_reference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With both models downloaded and ready, we create a function that will perform the summaries.\n",
    "\n",
    "The function takes fourth parameters:\n",
    "\n",
    "* the list of texts to summarize.\n",
    "* the tokenizer.\n",
    "* the model.\n",
    "* the maximum length for the generated summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-12T15:05:59.991764Z",
     "iopub.status.busy": "2023-09-12T15:05:59.99094Z",
     "iopub.status.idle": "2023-09-12T15:06:00.001952Z",
     "shell.execute_reply": "2023-09-12T15:06:00.000704Z",
     "shell.execute_reply.started": "2023-09-12T15:05:59.99172Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_summaries(texts_list, tokenizer, model, max_l=125):\n",
    "    \n",
    "    # We are going to add a prefix to each article to be summarized \n",
    "    # so that the model knows what it should do\n",
    "    prefix = \"Summarize this news: \"  \n",
    "    summaries_list = [] #Will contain all summaries\n",
    "\n",
    "    texts_list = [prefix + text for text in texts_list]\n",
    "    \n",
    "    for text in texts_list:\n",
    "        \n",
    "        summary=\"\"\n",
    "        \n",
    "        #calculate the encodings\n",
    "        input_encodings = tokenizer(text, \n",
    "                                    max_length=1024, \n",
    "                                    return_tensors='pt', \n",
    "                                    padding=True, \n",
    "                                    truncation=True)\n",
    "\n",
    "        # Generate summaries\n",
    "        with torch.no_grad():\n",
    "            output = model.generate(\n",
    "                input_ids=input_encodings.input_ids,\n",
    "                attention_mask=input_encodings.attention_mask,\n",
    "                max_length=max_l,  # Set the maximum length of the generated summary\n",
    "                num_beams=2,     # Set the number of beams for beam search\n",
    "                early_stopping=True\n",
    "            )\n",
    "            \n",
    "        #Decode to get the text\n",
    "        summary = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
    "        \n",
    "        #Add the summary to summaries list \n",
    "        summaries_list += summary\n",
    "    return summaries_list \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create the summaries, we call the 'create_summaries' function, passing both the news articles and the corresponding tokenizer and model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-12T15:06:00.004184Z",
     "iopub.status.busy": "2023-09-12T15:06:00.003767Z",
     "iopub.status.idle": "2023-09-12T15:06:31.841199Z",
     "shell.execute_reply": "2023-09-12T15:06:31.839433Z",
     "shell.execute_reply.started": "2023-09-12T15:06:00.004079Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creating the summaries for both models. \n",
    "summaries_small = create_summaries(articles, \n",
    "                                  tokenizer_small, \n",
    "                                  model_small)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-12T15:06:31.843742Z",
     "iopub.status.busy": "2023-09-12T15:06:31.843276Z",
     "iopub.status.idle": "2023-09-12T15:07:05.052825Z",
     "shell.execute_reply": "2023-09-12T15:07:05.051575Z",
     "shell.execute_reply.started": "2023-09-12T15:06:31.843702Z"
    }
   },
   "outputs": [],
   "source": [
    "summaries_reference = create_summaries(articles, \n",
    "                                      tokenizer_reference, \n",
    "                                      model_reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-12T15:07:05.055486Z",
     "iopub.status.busy": "2023-09-12T15:07:05.054786Z",
     "iopub.status.idle": "2023-09-12T15:07:05.063428Z",
     "shell.execute_reply": "2023-09-12T15:07:05.062089Z",
     "shell.execute_reply.started": "2023-09-12T15:07:05.055435Z"
    }
   },
   "outputs": [],
   "source": [
    "summaries_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-12T15:07:05.065645Z",
     "iopub.status.busy": "2023-09-12T15:07:05.064883Z",
     "iopub.status.idle": "2023-09-12T15:07:05.078376Z",
     "shell.execute_reply": "2023-09-12T15:07:05.077574Z",
     "shell.execute_reply.started": "2023-09-12T15:07:05.065609Z"
    }
   },
   "outputs": [],
   "source": [
    "summaries_reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first glance, it's evident that the summaries are different. \n",
    "\n",
    "However, it's challenging to determine which one is better. \n",
    "\n",
    "It's even difficult to discern whether they are significantly distinct or if there are just subtle differences between them.\n",
    "\n",
    "This is what we are going to verify now using ROUGE. When comparing the summaries of one model with those of the other, we don't get an idea of which one is better, but rather an idea of how much the summaries have changed with the fine-tuning applied to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROUGE\n",
    "Let's install and load all the necessary libraries to conduct a ROUGE evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-12T15:11:28.806753Z",
     "iopub.status.busy": "2023-09-12T15:11:28.806347Z",
     "iopub.status.idle": "2023-09-12T15:11:58.966857Z",
     "shell.execute_reply": "2023-09-12T15:11:58.965563Z",
     "shell.execute_reply.started": "2023-09-12T15:11:28.806724Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install evaluate\n",
    "!pip install rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-12T15:11:58.969817Z",
     "iopub.status.busy": "2023-09-12T15:11:58.969437Z",
     "iopub.status.idle": "2023-09-12T15:11:58.975447Z",
     "shell.execute_reply": "2023-09-12T15:11:58.974181Z",
     "shell.execute_reply.started": "2023-09-12T15:11:58.969779Z"
    }
   },
   "outputs": [],
   "source": [
    "import evaluate\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "#!pip install rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-12T15:07:05.991486Z",
     "iopub.status.idle": "2023-09-12T15:07:05.991893Z",
     "shell.execute_reply": "2023-09-12T15:07:05.991714Z",
     "shell.execute_reply.started": "2023-09-12T15:07:05.991695Z"
    }
   },
   "outputs": [],
   "source": [
    "#import evaluate\n",
    "#from nltk.tokenize import sent_tokenize\n",
    "#from rouge_score import rouge_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-12T15:12:30.861983Z",
     "iopub.status.busy": "2023-09-12T15:12:30.861518Z",
     "iopub.status.idle": "2023-09-12T15:12:32.323908Z",
     "shell.execute_reply": "2023-09-12T15:12:32.322793Z",
     "shell.execute_reply.started": "2023-09-12T15:12:30.861949Z"
    }
   },
   "outputs": [],
   "source": [
    "#With the function load of the library evaluate \n",
    "#we create a rouge_score object\n",
    "rouge_score = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating ROUGE is as simple as calling the *compute* function of the *rouge_score* object we created earlier. This function takes the texts to compare as arguments and a third value *use_stemmer*, which indicates whether it should use *stemmer* or full words for the comparison.\n",
    "\n",
    "A *stemmer* is the base of the word. Transform differents forms of a word in a same base. \n",
    "\n",
    "Some samples of steammer are: \n",
    "* Jumping -> Jump. \n",
    "* Running -> Run. \n",
    "* Cats -> Cat. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-12T15:12:36.52748Z",
     "iopub.status.busy": "2023-09-12T15:12:36.527065Z",
     "iopub.status.idle": "2023-09-12T15:12:36.534808Z",
     "shell.execute_reply": "2023-09-12T15:12:36.533627Z",
     "shell.execute_reply.started": "2023-09-12T15:12:36.527449Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_rouge_score(generated, reference):\n",
    "    \n",
    "    #We need to add '\\n' to each line before send it to ROUGE\n",
    "    generated_with_newlines = [\"\\n\".join(sent_tokenize(s.strip())) for s in generated]\n",
    "    reference_with_newlines = [\"\\n\".join(sent_tokenize(s.strip())) for s in reference]\n",
    "    \n",
    "    return rouge_score.compute(\n",
    "        predictions=generated_with_newlines,\n",
    "        references=reference_with_newlines,\n",
    "        use_stemmer=True,\n",
    "        \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-12T15:12:40.647148Z",
     "iopub.status.busy": "2023-09-12T15:12:40.646712Z",
     "iopub.status.idle": "2023-09-12T15:12:41.011346Z",
     "shell.execute_reply": "2023-09-12T15:12:41.009739Z",
     "shell.execute_reply.started": "2023-09-12T15:12:40.6471Z"
    }
   },
   "outputs": [],
   "source": [
    "compute_rouge_score(summaries_small, summaries_reference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there is a difference between the two models when performing summarization. \n",
    "\n",
    "For example, in ROUGE-1, the similarity is 47%, while in ROUGE-2, it's a 32%. This indicates that the results are different, with some similarities but differents enough. \n",
    "\n",
    "However, we still don't know which model is better since we have compared them to each other and not to a reference text. But at the very least, we know that the fine-tuning process applied to the second model has significantly altered its results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-07T17:21:28.296636Z",
     "iopub.status.busy": "2023-08-07T17:21:28.296173Z",
     "iopub.status.idle": "2023-08-07T17:21:28.301906Z",
     "shell.execute_reply": "2023-08-07T17:21:28.300702Z",
     "shell.execute_reply.started": "2023-08-07T17:21:28.296601Z"
    }
   },
   "source": [
    "# Comparing to a Dataset with real summaries. \n",
    "We are going to load the Dataset cnn_dailymail. This is a well-known dataset available in the **Datasets** library, and it suits our purpose perfectly. \n",
    "\n",
    "Apart from the news, it also contains pre-existing summaries. \n",
    "\n",
    "We will compare the summaries generated by the two models we are using with those from the dataset to determine which model creates summaries that are closer to the reference ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-12T15:12:45.797677Z",
     "iopub.status.busy": "2023-09-12T15:12:45.79725Z",
     "iopub.status.idle": "2023-09-12T15:16:53.672497Z",
     "shell.execute_reply": "2023-09-12T15:16:53.671301Z",
     "shell.execute_reply.started": "2023-09-12T15:12:45.79764Z"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "cnn_dataset = load_dataset(\n",
    "    \"cnn_dailymail\", version=\"3.0.0\"\n",
    ")\n",
    "\n",
    "#Get just a few news to test\n",
    "sample_cnn = cnn_dataset[\"test\"].select(range(MAX_NEWS))\n",
    "\n",
    "sample_cnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We retrieve the maximum length of the summaries to give the models the option to generate summaries of the same length, if they choose to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-12T15:17:02.366563Z",
     "iopub.status.busy": "2023-09-12T15:17:02.366072Z",
     "iopub.status.idle": "2023-09-12T15:17:02.37431Z",
     "shell.execute_reply": "2023-09-12T15:17:02.373015Z",
     "shell.execute_reply.started": "2023-09-12T15:17:02.366523Z"
    }
   },
   "outputs": [],
   "source": [
    "max_length = max(len(item['highlights']) for item in sample_cnn)\n",
    "max_length = max_length + 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-12T15:17:03.563215Z",
     "iopub.status.busy": "2023-09-12T15:17:03.562568Z",
     "iopub.status.idle": "2023-09-12T15:17:26.820027Z",
     "shell.execute_reply": "2023-09-12T15:17:26.818903Z",
     "shell.execute_reply.started": "2023-09-12T15:17:03.56318Z"
    }
   },
   "outputs": [],
   "source": [
    "summaries_t5_base = create_summaries(sample_cnn[\"article\"], \n",
    "                                      tokenizer_small, \n",
    "                                      model_small, \n",
    "                                      max_l=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-12T15:18:49.98889Z",
     "iopub.status.busy": "2023-09-12T15:18:49.988417Z",
     "iopub.status.idle": "2023-09-12T15:19:11.266176Z",
     "shell.execute_reply": "2023-09-12T15:19:11.264767Z",
     "shell.execute_reply.started": "2023-09-12T15:18:49.988854Z"
    }
   },
   "outputs": [],
   "source": [
    "summaries_t5_finetuned = create_summaries(sample_cnn[\"article\"], \n",
    "                                      tokenizer_reference, \n",
    "                                      model_reference, \n",
    "                                      max_l=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-12T15:19:37.145765Z",
     "iopub.status.busy": "2023-09-12T15:19:37.145324Z",
     "iopub.status.idle": "2023-09-12T15:19:37.152062Z",
     "shell.execute_reply": "2023-09-12T15:19:37.150764Z",
     "shell.execute_reply.started": "2023-09-12T15:19:37.145733Z"
    }
   },
   "outputs": [],
   "source": [
    "#Get the real summaries from the cnn_dataset\n",
    "real_summaries = sample_cnn['highlights']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the generated summaries alongside the reference summaries provided by the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-12T15:19:40.580043Z",
     "iopub.status.busy": "2023-09-12T15:19:40.579644Z",
     "iopub.status.idle": "2023-09-12T15:19:40.595382Z",
     "shell.execute_reply": "2023-09-12T15:19:40.594207Z",
     "shell.execute_reply.started": "2023-09-12T15:19:40.580013Z"
    }
   },
   "outputs": [],
   "source": [
    "summaries = pd.DataFrame.from_dict(\n",
    "        {\n",
    "            \"base\": summaries_t5_base, \n",
    "            \"finetuned\": summaries_t5_finetuned,\n",
    "            \"reference\": real_summaries,\n",
    "        }\n",
    "    )\n",
    "summaries.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can calculate the ROUGE scores for the two models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-12T15:19:43.333808Z",
     "iopub.status.busy": "2023-09-12T15:19:43.333406Z",
     "iopub.status.idle": "2023-09-12T15:19:43.591965Z",
     "shell.execute_reply": "2023-09-12T15:19:43.590757Z",
     "shell.execute_reply.started": "2023-09-12T15:19:43.333778Z"
    }
   },
   "outputs": [],
   "source": [
    "compute_rouge_score(summaries_t5_base, real_summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-12T15:19:45.55926Z",
     "iopub.status.busy": "2023-09-12T15:19:45.558785Z",
     "iopub.status.idle": "2023-09-12T15:19:45.80719Z",
     "shell.execute_reply": "2023-09-12T15:19:45.805976Z",
     "shell.execute_reply.started": "2023-09-12T15:19:45.559222Z"
    }
   },
   "outputs": [],
   "source": [
    "compute_rouge_score(summaries_t5_finetuned, real_summaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these results, I would say that the fine-tuned model performs slightly better than the T5-Base model. It consistently achieves higher ROUGE scores in all metrics except for LSUM, where the difference is minimal.\n",
    "\n",
    "Additionally, the ROUGE metrics are quite interpretable. \n",
    "\n",
    "LSUM indicates the percentage of the longest common subsequence, regardless of word order, in relation to the total length of the text. \n",
    "\n",
    "This can be a good indicator of overall similarity between texts. However, both models have very similar LSUM scores, and the fine-tuned model has better scores in other ROUGE metrics.\n",
    "\n",
    "Personally, I would lean towards the fine-tuned model, although the difference may not be very significant.\n",
    "\n",
    "## Continue learning\n",
    "This notebook is part of a [course on large language models](https://github.com/peremartra/Large-Language-Model-Notebooks-Course) I'm working on and it's available on [GitHub](https://github.com/peremartra/Large-Language-Model-Notebooks-Course). You can see the other lessons and if you like it, don't forget to subscribe to receive notifications of new lessons.\n",
    "\n",
    "Other notebooks in the Large Language Models series: \n",
    "https://www.kaggle.com/code/peremartramanonellas/use-a-vectorial-db-to-optimize-prompts-for-llms\n",
    "https://www.kaggle.com/code/peremartramanonellas/ask-your-documents-with-langchain-vectordb-hf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feel Free to fork or edit the noteboook for you own convenience. Please consider ***UPVOTING IT***. It helps others to discover the notebook, and it encourages me to continue publishing."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
