{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/peremartra/Large-Language-Model-Notebooks-Course/blob/main/P2-MHF/Aligning_DPO_open_llama_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pa8905-YsHAn"
      },
      "source": [
        "<div align=\"center\">\n",
        "<h1><a href=\"https://github.com/peremartra/Large-Language-Model-Notebooks-Course\">Learn by Doing LLM Projects</a></h1>\n",
        "    <h3>Understand And Apply Large Language Models</h3>\n",
        "    <h2>Creating and Publishing Your Own LLM.</h2>\n",
        "    <h3>Aligning with DPO a phi3-3 model.</h3>\n",
        "    by <b>Pere Martra</b>\n",
        "</div>\n",
        "\n",
        "<br>\n",
        "\n",
        "<div align=\"center\">\n",
        "    &nbsp;\n",
        "    <a target=\"_blank\" href=\"https://www.linkedin.com/in/pere-martra/\"><img src=\"https://img.shields.io/badge/style--5eba00.svg?label=LinkedIn&logo=linkedin&style=social\"></a>\n",
        "    \n",
        "</div>\n",
        "\n",
        "<br>\n",
        "<hr>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this Notebook we are going to align a Microsoft Phi-3 Model using DPO, and publish it to Hugging Face!\n",
        "\n",
        "Base Model: https://huggingface.co/microsoft/Phi-3-mini-4k-instruct\n",
        "\n",
        "Dataset Used: https://huggingface.co/datasets/argilla/distilabel-capybara-dpo-7k-binarized\n",
        "\n",
        "Model obtained: https://huggingface.co/oopere/martra-phi-3-mini-dpo\n",
        "\n",
        "## To start, a brief introduction to DPO.\n",
        "\n",
        "The revolution we're currently experiencing around Large Language Models began with the emergence of ChatGPT and its GPT-3.5 model.\n",
        "\n",
        "Something different had been done with GPT-3.5, which was actually a derivative of GPT-3, a model that did not generate nearly as much excitement as its successor.\n",
        "\n",
        "Many people, including myself, believe that the main difference was the use of Alignemet using RLHF - Reinforcement Learning from Human Feedback.\n",
        "\n",
        "Nowadays RLHF has been displaced by a technique that achieves the same result in a much more efficient way: DPO - Direct Preference Optimization.\n",
        "\n",
        "Both DPO and RLHF are alignment techniques that require a dataset containing correct and incorrect responses to the same prompt.\n",
        "\n",
        "But from here, the differences begin. RLHF uses this dataset to train a second model, called a reward model, which will be used in the alignment process. DPO, on the other hand, uses the dataset directly to train the final model. This is the main difference between the two techniques.\n",
        "\n",
        "As you can imagine, DPO is a more direct technique that requires fewer resources. When we're talking about models with tens of billions of parameters, any reduction in resource consumption can result in significant cost savings.\n",
        "\n",
        "The implementation of DPO that we are going to use is the one developed by Hugging Face in their TRL library, which stands for Transformer Reinforcement Learning. DPO can be considered a reinforcement learning technique, where the model is rewarded during its training phase based on its responses.\n",
        "\n",
        "_______________________\n",
        "\n",
        "Since is necesary to save the created model, the notebook mounts a disk in  Google Drive. If you run it locally, you don't need to execute this line of code. You can actually also run it on Google Colab without mounting a disk in your drive, but then every time you close the session you'll lose the saved model, as it will be saved in the temporary directory of the Google Colab session."
      ],
      "metadata": {
        "id": "wvXF2mOzXT5C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OsiXAcb-PDwq",
        "outputId": "07522fe9-1225-420b-dd8d-68a461f01869"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that the disk is mounted, it's time to load the necessary libraries:\n",
        "\n",
        "The only one that might be new to you is the trl library, which stands for Transformer Reinforcement Learning. You'll be importing the DPOTrainer class from this library, which you'll use to perform the DPO fine-tuning of the model."
      ],
      "metadata": {
        "id": "oN9tG_fqYeXR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_zIBL8IssExG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "292882dc-b664-4c17-9733-1c38193ad447"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.2/245.2 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m314.1/314.1 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.4/103.4 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q datasets==2.19.1\n",
        "!pip install -q trl==0.8.6\n",
        "!pip install -q peft==0.11.1\n",
        "!pip install -q transformers==4.41.0\n",
        "!pip install -q bitsandbytes==0.43.1\n",
        "!pip install -q sentencepiece==0.1.99\n",
        "!pip install -q accelerate==0.30.1\n",
        "!pip install -q huggingface_hub==0.23.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YpdkZsMNylvp"
      },
      "outputs": [],
      "source": [
        "#Import necessary classes.\n",
        "import gc\n",
        "import torch\n",
        "\n",
        "import transformers\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from transformers import TrainingArguments, BitsAndBytesConfig\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig, get_peft_model, PeftModel\n",
        "from trl import DPOTrainer\n",
        "import bitsandbytes as bnb\n",
        "\n",
        "from getpass import getpass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "_K6m33GuYsFd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another necessary step is to log in to Hugging Face."
      ],
      "metadata": {
        "id": "OTXdkOgrZtYe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TQFg7BX3h4Hu"
      },
      "outputs": [],
      "source": [
        "hf_token = getpass(\"Hugging Face: \")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login --token $hf_token"
      ],
      "metadata": {
        "id": "S8fB974aZtjd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8CvUgROUDw-"
      },
      "source": [
        "## Format dataset\n",
        "\n",
        "The model I've chosen is the Microsoft Phi-3 mini with the 4k context. It's a 3.8B parameter model that is very competitive and in many cases outperforms 7B parameter models.\n",
        "I've chosen a small model so that its training can be done with few resources on Google Colab or on a not very large GPU.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"google/gemma-2b-it\"\n",
        "new_model = \"martra-open-gemma-2b-it-dpo\""
      ],
      "metadata": {
        "id": "KIY4j-3ebylP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "id": "slQKPKiKXMaQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before you begin training the model, is necesary need to load the dataset and transform it to fit the format required by the DPOTrainer class, that consists of three fields: the prompt, the chosen answer, and a discarded answer.\n",
        "\n",
        "I'm loading just a fer rows of the Dataset, feel free to use the full Dataset if you have enough time.\n",
        "\n",
        "You may need more or less 2 hours of a A100 GPU to train with Full Dataset for a 20 epochs. If you use the configuration in the notebook you are going to need just 40 minutes in a L4 GPU."
      ],
      "metadata": {
        "id": "P3DqIU4jcG37"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "dataset_original =  load_dataset(\"argilla/distilabel-capybara-dpo-7k-binarized\",\n",
        "                                 split='train[500:]')\n",
        "dataset_eval = load_dataset(\"argilla/distilabel-capybara-dpo-7k-binarized\",\n",
        "                            split='train[:500]')\n",
        "\n",
        "# Save columns\n",
        "original_columns = dataset_original.column_names\n",
        "print(original_columns)"
      ],
      "metadata": {
        "id": "zJoenkTlgBbS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_original"
      ],
      "metadata": {
        "id": "vrto5-0EC0lq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset has many more columns than are actually necessary for the DPO process. However, I'm going to take advantage of a couple of them to filter the data to be used."
      ],
      "metadata": {
        "id": "SFus-k9DcU-7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_filtered = dataset_original.filter(\n",
        "  lambda r: r[\"rating_chosen\"]>=4.0 and r[\"rating_rejected\"] <= 2.5\n",
        ")"
      ],
      "metadata": {
        "id": "ciwb-oRd1o-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This first filter only retrieves those rows where the rating of the chosen response is very high and the rating of the discarded responses is very low. This is a way to facilitate the model's learning, although it's also possible that it doesn't help in the last epochs of training.\n",
        "\n",
        "I'm going to perform a second filter to keep the prompt length under control, as the selected model only accepts a length of 4000 tokens."
      ],
      "metadata": {
        "id": "6FBFMfXXdBoY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_filtered = dataset_filtered.map(lambda r: {\"messages\": len(r[\"chosen\"])}).filter(lambda r: r[\"messages\"]<3) #and len(r[\"prompt\"]) + len(r[\"chosen\"]) + len(r[\"rejected\"]) < 3800)\n"
      ],
      "metadata": {
        "id": "p5oZFJPNltc8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "Cl9sOYUZcbS4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_filtered"
      ],
      "metadata": {
        "id": "bXjgb5zo237x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset still have all the columns, but the number of rows has been significantly reduced. Let me warn you that 169 rows are too few to perform proper training; again, this reduction is so that will be possible to execute the notebook in just some minutes, and obtain results."
      ],
      "metadata": {
        "id": "khFPFXrUdK2w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "rP1ifwqRPMNX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Repeat the same filters with the Validation Dataset.\n",
        "dataset_eval_filtered = dataset_eval.filter(\n",
        "  lambda r: r[\"rating_chosen\"]>=4.0 and r[\"rating_rejected\"] <= 2.5\n",
        ")\n",
        "dataset_eval_filtered = dataset_eval_filtered.map(lambda r: {\"messages\": len(r[\"chosen\"])}).filter(lambda r: r[\"messages\"]<3 )#and len(r[\"prompt\"]) + len(r[\"chosen\"]) + len(r[\"rejected\"]) < 3800)\n",
        "dataset_eval_filtered"
      ],
      "metadata": {
        "id": "5nd47fwHFc6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, it's a matter of creating a function to adapt the dataset's structure to what's required by the DPOTraining class.\n",
        "\n",
        "I have to confess that I've cheated a little bit. The function comes from the Hugging Face dataset card. I only had to remove an error that they had missed.\n",
        "\n",
        "In summary, the function takes a row and retrieves only the three necessary columns. It also applies a small format to the responses, which I've adapted to the format required by the Model, adding the label <|end|> after the responses.\n"
      ],
      "metadata": {
        "id": "OL_tMsqCdkKR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chatml_format(example):\n",
        "    # get everything except the last message as input\n",
        "    prompt = tokenizer.apply_chat_template(example[\"chosen\"][:-1], tokenize=False,\n",
        "                                           add_generation_prompt=True)\n",
        "    # get the last assistant responses\n",
        "    chosen = example[\"chosen\"][-1][\"content\"] + \"<end_of_turn>\\n\"\n",
        "    rejected = example[\"rejected\"][-1][\"content\"] + \"<end_of_turn>\\n\"\n",
        "\n",
        "    return {\n",
        "        \"prompt\": prompt,\n",
        "        \"chosen\": chosen,\n",
        "        \"rejected\": rejected,\n",
        "    }"
      ],
      "metadata": {
        "id": "BhTHhLc5W1vK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I'll use the dataset's map function to execute the transformation on each row, and also remove the original columns."
      ],
      "metadata": {
        "id": "cpyHl0iMdz50"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Format dataset\n",
        "dataset = dataset_filtered.map(\n",
        "    chatml_format,\n",
        "    remove_columns=dataset_filtered.column_names\n",
        ")\n",
        "# Print sample\n",
        "dataset[12]"
      ],
      "metadata": {
        "id": "x2t91EEhXexx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Format dataset\n",
        "dataset_eval = dataset_eval_filtered.map(\n",
        "    chatml_format,\n",
        "    remove_columns=original_columns\n",
        ")"
      ],
      "metadata": {
        "id": "ohm56CB4xx21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print sample\n",
        "dataset_eval[20]"
      ],
      "metadata": {
        "id": "sNp5nfsVj3sd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The format is adapted to chat in phi3:\n",
        "https://huggingface.co/microsoft/Phi-3-mini-128k-instruct\n",
        "\n",
        "\n",
        "\n",
        "<|user|>\n",
        "I am going to Paris, what should I see?<|end|>\n",
        "\n",
        "<|assistant|>\n",
        "Paris, the capital of France, is known for its stunning architecture, art museums, historical landmarks, and romantic atmosphere. Here are some of the top attractions to see in Paris:\\n\\n1. The Eiffel Tower: The iconic Eiffel Tower is one of the most recognizable landmarks in the world and offers breathtaking views of the city.\\n2. The Louvre Museum: The Louvre is one of the world's largest and most famous museums, housing an impressive collection of art and artifacts, including the Mona Lisa.\\n3. Notre-Dame Cathedral: This beautiful cathedral is one of the most famous landmarks in Paris and is known for its Gothic architecture and stunning stained glass windows.\\n\\nThese are just a few of the many attractions that Paris has to offer. With so much to see and do, it's no wonder that Paris is one of the most popular tourist destinations in the world.\"<|end|>\n",
        "\n",
        "<|user|>\n",
        "What is so great about #1?<|end|>\n",
        "\n"
      ],
      "metadata": {
        "id": "dTl_kLIYaObV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "id": "zYcv1zG6zLQc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_eval"
      ],
      "metadata": {
        "id": "uuxyyl-6zNoL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DeT5eUK_UJgK"
      },
      "source": [
        "## Train model with DPO"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Finetuning with DPOTrainer.\n",
        "\n",
        "Time to start working with the necessary configurations to perform alignment using DPO.\n",
        "\n"
      ],
      "metadata": {
        "id": "tepNZiTzfz9I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LoRA configuration\n",
        "peft_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    #target_modules=['o_proj', 'qkv_proj'] #phi-3\n",
        "    target_modules=\"all-linear\"\n",
        ")"
      ],
      "metadata": {
        "id": "zhUasRuGNiGo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The value of **r** indicates the size of the reparameterization; the higher the value, the more parameters are trained. An 8 is at the upper limit of what is recommended for small models.\n",
        "\n",
        "To further accentuate the weight of the new training, I use the **lora_alpha** value. It's a multiplier that adjusts the layers inserted by LoRA. Normally it's left at 1, but in the case of DPO, I've seen values as high as 128.\n",
        "\n",
        "The recommendation is that **lora_alpha** should be double the value of **r**. Since **r** varies depending on the model size, you may end up with a very high lora_alpha value if you want to fine-tune a large model and, for example, specify an **r** of 64."
      ],
      "metadata": {
        "id": "_KazYN5Neja0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16\n",
        ")"
      ],
      "metadata": {
        "id": "_R6T4Kjy_gmS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The quantization configuration holds no secrets, we are reducing the model's precision to 4 bits."
      ],
      "metadata": {
        "id": "KcGgcdkoe_0n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model to fine-tune\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    torch_dtype=torch.bfloat16\n",
        ")\n",
        "model.config.use_cache = False"
      ],
      "metadata": {
        "id": "KAugJ93ps3nW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next step is to create the training parameters."
      ],
      "metadata": {
        "id": "rrOaJiZ3fRsu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rKPILNOLR-aK"
      },
      "outputs": [],
      "source": [
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    per_device_train_batch_size=1,\n",
        "    per_device_eval_batch_size=1,\n",
        "    gradient_accumulation_steps=3,\n",
        "    gradient_checkpointing=True,\n",
        "    remove_unused_columns=True,\n",
        "    learning_rate=5.0e-06,\n",
        "    eval_strategy=\"epoch\",\n",
        "    logging_strategy=\"epoch\",\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    num_train_epochs=6,\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_steps=1,\n",
        "    output_dir=new_model,\n",
        "    optim=\"paged_adamw_32bit\",\n",
        "    warmup_steps=2,\n",
        "    bf16=True,\n",
        "    report_to=\"none\",\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I'll try to explain the value of the most important and specific ones.\n",
        "\n",
        "**lr_scheduler_type**=\"cosine\": The learning rate is adjusted according to a cosine schedule. It starts at the value specified in learning_rate and then gradually decreases.\n",
        "\n",
        "**warmup_steps**=2:  For the first two epochs, the learning rate is adjusted by increasing its value instead of decreasing it. The aim is to stabilize the learning process.\n",
        "\n",
        "**Gradient_accumulation_steps**=2: To save memory. I accumulate the gradients over two steps before updating the model weights.\n",
        "\n",
        "With these parameters, I've tried to find a training setup with low memory requirements, thanks to the use of gradient accumulation, gradient checkpointing, a small batch size, and the use of bf16 along with the paged_adamw_32bit optimizer."
      ],
      "metadata": {
        "id": "0VWJ5uS-fYN5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create DPO trainer\n",
        "trainer = DPOTrainer(\n",
        "    model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset,\n",
        "    eval_dataset=dataset_eval,\n",
        "    tokenizer=tokenizer,\n",
        "    peft_config=peft_config,\n",
        "    beta=0.1,\n",
        "    max_prompt_length=2048,\n",
        "    max_length=2048,\n",
        ")"
      ],
      "metadata": {
        "id": "nw8BR1_vC2rF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The parameters for DPOTrainer are quite simple. You need to pass it the configurations you've created, the two evaluation datasets, and the maximum length of the prompt and response.\n",
        "\n",
        "The indicated beta value is a standard that balances the new training with the model's base knowledge. If you want the new training to have more weight, perhaps because you're training for a very specific task, you could specify a lower beta value."
      ],
      "metadata": {
        "id": "rEVpneD8gCjC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fine-tune model with DPO\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "5kZCQsGVC02f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It seems to have worked reasonably well, although there might be a potential overfitting issue, where the model adapts better to the training data than to the evaluation data. To mitigate overfitting, you could expand the dataset and try increasing the **lora_dropout** parameter in **LoraConfig**.\n"
      ],
      "metadata": {
        "id": "4innMfD7gPAL"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LdhPpcrUM3H"
      },
      "source": [
        "## Upload model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#PATH_MODEL=\"/content/drive/MyDrive/final_checkpoint\"\n",
        "PATH_MODEL=\"/content/drive/MyDrive/apress_checkpoint\""
      ],
      "metadata": {
        "id": "Uj_gNWIbD3c9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save artifacts\n",
        "trainer.model.save_pretrained(PATH_MODEL)\n",
        "tokenizer.save_pretrained(PATH_MODEL)\n",
        "\n"
      ],
      "metadata": {
        "id": "jN4nyQC0KgGL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Execute this cell only if you are having memory issues. (Not you, of course, I mean your environment 🤗)."
      ],
      "metadata": {
        "id": "dzEh5ZJZgcpd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Flush memory\n",
        "#del dpo_trainer, model\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "beJHHDBgla5i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, you're going to load the original model again, but this time in its unquantized format."
      ],
      "metadata": {
        "id": "FBh-U1mSgb4E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h7cIvxcTfBC4"
      },
      "outputs": [],
      "source": [
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    return_dict=True,\n",
        "    torch_dtype=torch.bfloat16,\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name,\n",
        "                                          use_fast=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The original model and the saved training are being merged."
      ],
      "metadata": {
        "id": "rIIv9Higg_8H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = PeftModel.from_pretrained(base_model, PATH_MODEL)\n",
        "model = model.merge_and_unload()"
      ],
      "metadata": {
        "id": "W4yTSZmQM94u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " The model that you have in memory is now a combination of the base model and the adapter that you have trained. You can now save this new model and upload it to Hugging Face."
      ],
      "metadata": {
        "id": "0pXaNygrJvgW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(new_model)\n",
        "tokenizer.save_pretrained(new_model)"
      ],
      "metadata": {
        "id": "wUiAMpplNIft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.push_to_hub(new_model,\n",
        "                  private=True,\n",
        "                  use_temp_dir=False,\n",
        "                  token=hf_token)\n",
        "tokenizer.push_to_hub(new_model,\n",
        "                      private=True,\n",
        "                      use_temp_dir=False,\n",
        "                      token=hf_token)"
      ],
      "metadata": {
        "id": "i43wf3emOt-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6EFsmS4UOgV"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's test the new model and compare with the original"
      ],
      "metadata": {
        "id": "ZfA5T3K-J71j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Format prompt\n",
        "message = [\n",
        "    {\"role\": \"user\", \"content\": \"3713841893836/4?\\nLimit your response to mathematical expressions and symbols.\"}\n",
        "]"
      ],
      "metadata": {
        "id": "gNl8jsclLAN8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#new_model=\"oopere/martra-test-phi-3-mini-dpo\"\n",
        "tokenizer_new_model = AutoTokenizer.from_pretrained(new_model)\n",
        "prompt = tokenizer_new_model.apply_chat_template(message, add_generation_prompt=True, tokenize=False)\n",
        "\n",
        "# Create pipeline\n",
        "pipeline_new = transformers.pipeline(\n",
        "    \"text-generation\",\n",
        "    model=new_model,\n",
        "    tokenizer=tokenizer_new_model\n",
        ")"
      ],
      "metadata": {
        "id": "6DW3zSQsVwuX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate text\n",
        "sequences = pipeline_new(\n",
        "    prompt,\n",
        "    do_sample=True,\n",
        "    temperature=0.1,\n",
        "    top_p=0.2,\n",
        "    num_return_sequences=1,\n",
        "    max_length=200,\n",
        ")\n",
        "print(sequences[0]['generated_text'])"
      ],
      "metadata": {
        "id": "QNzczoVgV_uh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PERFECT! The new model only returns numbers, as we want!\n",
        "\n",
        "If you don't want to wait to the training, just test my model on hugging Face. It has been trained with the same Dataset for 2 hours in a A100 GPU on Colab."
      ],
      "metadata": {
        "id": "KlzRy6R6KCdL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LAEUZFjvlJOv"
      },
      "outputs": [],
      "source": [
        "#Test my Model on hugging Face.\n",
        "new_model=\"oopere/martra-phi-3-mini-dpo\"\n",
        "tokenizer_new_model = AutoTokenizer.from_pretrained(new_model)\n",
        "prompt = tokenizer_new_model.apply_chat_template(message, add_generation_prompt=True, tokenize=False)\n",
        "\n",
        "# Create pipeline\n",
        "pipeline_new = transformers.pipeline(\n",
        "    \"text-generation\",\n",
        "    model=new_model,\n",
        "    tokenizer=tokenizer_new_model\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate text\n",
        "sequences = pipeline_new(\n",
        "    prompt,\n",
        "    do_sample=True,\n",
        "    temperature=0.1,\n",
        "    top_p=0.2,\n",
        "    num_return_sequences=1,\n",
        "    max_length=200,\n",
        ")\n",
        "print(sequences[0]['generated_text'])"
      ],
      "metadata": {
        "id": "zxouPKEcK91X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary\n",
        "\n",
        "The model alignment process has been a complete success. The truth is, with the Hugging Face libraries, everything is straightforward.\n",
        "\n",
        "The challenge is knowing about the technique, when to apply it, and having the necessary data.\n",
        "\n",
        "In this notebook, you've addressed the first two points.\n",
        "\n",
        "I got a lot of inspiration from:\n",
        "\n",
        "* [RLHF in 2024 with DPO & Hugging Face](https://www.philschmid.de/dpo-align-llms-in-2024-with-trl) by Phil Schmid.\n",
        "\n",
        "* [Fine-tune a Mistral-7b model with Direct Preference Optimizatio](https://medium.com/towards-data-science/fine-tune-a-mistral-7b-model-with-direct-preference-optimization-708042745aac) by Maxime Labonne\n",
        "\n"
      ],
      "metadata": {
        "id": "M3iVcWtzKm0g"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tBq7Crg3wqtK"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}