{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<div align=\"center\">\n",
    "<h1><a href=\"https://github.com/peremartra/Large-Language-Model-Notebooks-Course\">LLM Hands On Course</a></h1>\n",
    "    <h3>Understand And Apply Large Language Models</h3>\n",
    "    <h2>Introduction to Prompt Tuning using PEFT from Hugging Face.</h2>\n",
    "    <h3>Fine-tune a Foundational Model effortless</h3>\n",
    "    <p>by <b>Pere Martra</b></p>\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "<div align=\"center\">\n",
    "    &nbsp;\n",
    "    <a target=\"_blank\" href=\"https://www.linkedin.com/in/pere-martra/\"><img src=\"https://img.shields.io/badge/style--5eba00.svg?label=LinkedIn&logo=linkedin&style=social\"></a>\n",
    "    \n",
    "</div>\n",
    "\n",
    "<br>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6fba2d42-ed99-4a03-8033-d479ce24d5dd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Prompt Tuning\n",
    "In this notebook I'm introducing how to apply prompt tuning with the PEFT library to a foundational model. \n",
    "\n",
    "For a complete list of Models compatible with PEFT refer to their [documentation](https://huggingface.co/docs/peft/main/en/index#supported-methods). \n",
    "\n",
    "A short sample of models available to be trained with PEFT are: Bloom, Llama, GPT-J, GPT-2, BERT... and more. Hugging Face is working hard to bring more Models to the Library. \n",
    "\n",
    "## Brief introduction to Prompt Tuning. \n",
    "Prompt Tuning, or Soft Prompt, is an Additive training Technique. We don't modify the weights of the model, instead we modify the weights of the prompt. To achieve that, we must add some new values to the prompt, and these values are trained. We only modify the weights of the new values in the layers containing the prompt. \n",
    "\n",
    "We can modify the behavior of a model by just updating 0.0005% of their weights. Achieving a similar result to other techniques where we update the weights of the model.  \n",
    "\n",
    "The training is Faster and Cheaper. And not only that, we can train different models and in inference time, we just need to load one foundational model, together with the new small trained models because the weights of the original have not been modified. \n",
    "\n",
    "## what are we going to do in the notebook\n",
    "We are going to train two different models using two datasets and just one Pre Trained model from the Bloom family. A model will be trained with a Dataset of prompts and the other with a Dataset of inspirational sentences. We will compare the results to the same question of the models before and after training. \n",
    "\n",
    "We will see how we can load both models having just one copy of the foundational Model in Memory. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Peft Library\n",
    "This library contains the Hugging Face implementation os differente fine-tuning techniques, like Prompt Tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d16bf5ec-888b-4c76-a655-193fd4cc8a36",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: peft in /Users/pere/miniforge3/envs/tfphy11/lib/python3.11/site-packages (0.4.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/pere/miniforge3/envs/tfphy11/lib/python3.11/site-packages (from peft) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/pere/miniforge3/envs/tfphy11/lib/python3.11/site-packages (from peft) (23.1)\n",
      "Requirement already satisfied: psutil in /Users/pere/miniforge3/envs/tfphy11/lib/python3.11/site-packages (from peft) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /Users/pere/miniforge3/envs/tfphy11/lib/python3.11/site-packages (from peft) (6.0)\n",
      "Requirement already satisfied: torch>=1.13.0 in /Users/pere/miniforge3/envs/tfphy11/lib/python3.11/site-packages (from peft) (2.1.0.dev20230825)\n",
      "Requirement already satisfied: transformers in /Users/pere/miniforge3/envs/tfphy11/lib/python3.11/site-packages (from peft) (4.31.0)\n",
      "Requirement already satisfied: accelerate in /Users/pere/miniforge3/envs/tfphy11/lib/python3.11/site-packages (from peft) (0.22.0)\n",
      "Requirement already satisfied: safetensors in /Users/pere/miniforge3/envs/tfphy11/lib/python3.11/site-packages (from peft) (0.3.1)\n",
      "Requirement already satisfied: filelock in /Users/pere/miniforge3/envs/tfphy11/lib/python3.11/site-packages (from torch>=1.13.0->peft) (3.12.2)\n",
      "Requirement already satisfied: typing-extensions in /Users/pere/miniforge3/envs/tfphy11/lib/python3.11/site-packages (from torch>=1.13.0->peft) (4.5.0)\n",
      "Requirement already satisfied: sympy in /Users/pere/miniforge3/envs/tfphy11/lib/python3.11/site-packages (from torch>=1.13.0->peft) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/pere/miniforge3/envs/tfphy11/lib/python3.11/site-packages (from torch>=1.13.0->peft) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/pere/miniforge3/envs/tfphy11/lib/python3.11/site-packages (from torch>=1.13.0->peft) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /Users/pere/miniforge3/envs/tfphy11/lib/python3.11/site-packages (from torch>=1.13.0->peft) (2023.6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /Users/pere/miniforge3/envs/tfphy11/lib/python3.11/site-packages (from transformers->peft) (0.16.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/pere/miniforge3/envs/tfphy11/lib/python3.11/site-packages (from transformers->peft) (2023.6.3)\n",
      "Requirement already satisfied: requests in /Users/pere/miniforge3/envs/tfphy11/lib/python3.11/site-packages (from transformers->peft) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/pere/miniforge3/envs/tfphy11/lib/python3.11/site-packages (from transformers->peft) (0.13.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/pere/miniforge3/envs/tfphy11/lib/python3.11/site-packages (from transformers->peft) (4.65.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/pere/miniforge3/envs/tfphy11/lib/python3.11/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/pere/miniforge3/envs/tfphy11/lib/python3.11/site-packages (from requests->transformers->peft) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/pere/miniforge3/envs/tfphy11/lib/python3.11/site-packages (from requests->transformers->peft) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/pere/miniforge3/envs/tfphy11/lib/python3.11/site-packages (from requests->transformers->peft) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/pere/miniforge3/envs/tfphy11/lib/python3.11/site-packages (from requests->transformers->peft) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/pere/miniforge3/envs/tfphy11/lib/python3.11/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install peft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the transformers library we import the necesary classes to import the model and the tokenizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "31738463-c9b0-431d-869e-1735e1e2f5c7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the model and the tokenizers. \n",
    "\n",
    "Bloom is one of the smallest and smarter model available to be trained with PEFT Library using Prompt Tuning. You can use either of the models in the Bloom Family, I encorage you to use at least two of them and see the differences. \n",
    "\n",
    "I'm using the smallest one just to spend less time trainig, and avoid memory problems in Colab. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bigscience/bloomz-560m\"\n",
    "#model_name=\"bigscience/bloom-1b1\"\n",
    "NUM_VIRTUAL_TOKENS = 4\n",
    "NUM_EPOCHS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "foundational_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference with the pre trained bloom model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function returns the outputs from the model received, and inputs. \n",
    "def get_outputs(model, inputs, max_new_tokens=100):\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"], \n",
    "        max_new_tokens=max_new_tokens, \n",
    "        repetition_penalty=1.5, #Avoid repetition. \n",
    "        early_stopping=True, #The model can stop before reach the max_length\n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ca4d203a-5152-4947-ab34-cfd0b40a102a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Before doing any fine-tuning, we will ask the models to generate a new phrase to the following input sentence.\n",
    "\n",
    "As we want to have 2 different trained models i will create two diferent prompts. \n",
    "\n",
    "The first Model will be trained with a Dataset with prompts, and teh second one with a Dataset with motivation sentences.  \n",
    "\n",
    "The first model will recieve the prompt \"I want you to act as an English translator, \" and the second Model \"There two thing that matter:\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1d4c80a9-4edd-4fcd-aef0-996f4da5cc02",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"I want you to act as a motivational coach.  Don't be afraid of being challenged.\"]\n"
     ]
    }
   ],
   "source": [
    "input_prompt = tokenizer(\"I want you to act as a motivational coach. \", return_tensors=\"pt\")\n",
    "foundational_outputs_prompt = get_outputs(foundational_model, input_prompt, max_new_tokens=50)\n",
    "\n",
    "print(tokenizer.batch_decode(foundational_outputs_prompt, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['There two thing that matter: the size and shape of a flower']\n"
     ]
    }
   ],
   "source": [
    "input_sentences = tokenizer(\"There two thing that matter:\", return_tensors=\"pt\")\n",
    "foundational_outputs_sentence = get_outputs(foundational_model, input_sentences, max_new_tokens=50)\n",
    "\n",
    "print(tokenizer.batch_decode(foundational_outputs_sentence, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f438d43b-6b9f-445e-9df4-60ea09640764",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Both answer are more or less correct. Any of the Bloom models is pre-trained and can generate sentences correctly, and with sense. Let's see if after training the reponses are equal or more or less acurated. \n",
    "\n",
    "## Preparing the Datasets\n",
    "The Datasets useds are: \n",
    "* https://huggingface.co/datasets/fka/awesome-chatgpt-prompts\n",
    "* https://huggingface.co/datasets/Abirate/english_quotes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2ed62b41-e3fa-4a41-a0a9-59f35a6904f9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['prompt', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 50\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_prompt = \"fka/awesome-chatgpt-prompts\"\n",
    "\n",
    "#Create the Dataset to create prompts. \n",
    "data_prompt = load_dataset(dataset_prompt)\n",
    "data_prompt = data_prompt.map(lambda samples: tokenizer(samples[\"prompt\"]), batched=True)\n",
    "train_sample_prompt = data_prompt[\"train\"].select(range(50))\n",
    "\n",
    "train_sample_prompt = train_sample_prompt.remove_columns('act')\n",
    "\n",
    "display(train_sample_prompt) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['quote', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 50\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_sentences = load_dataset(\"Abirate/english_quotes\")\n",
    "\n",
    "data_sentences = dataset_sentences.map(lambda samples: tokenizer(samples[\"quote\"]), batched=True)\n",
    "train_sample_sentences = data_sentences[\"train\"].select(range(50))\n",
    "train_sample_sentences = train_sample_sentences.remove_columns(['author', 'tags'])\n",
    "display(train_sample_sentences) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b97381d4-5fe2-49d0-be5d-2fe3421edc5c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## fine-tuning.  \n",
    "\n",
    "### PEFT configurations \n",
    "\n",
    "\n",
    "API docs:\n",
    "https://huggingface.co/docs/peft/main/en/package_reference/tuners#peft.PromptTuningConfig\n",
    "\n",
    "We can use the same configuration for both models to be trained. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6df8e1f1-be9e-42db-b4a4-6af7cd351004",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from peft import  get_peft_model, PromptTuningConfig, TaskType, PromptTuningInit\n",
    "\n",
    "generation_config = PromptTuningConfig(\n",
    "    task_type=TaskType.CAUSAL_LM, #This type indicates the model will generate text. \n",
    "    prompt_tuning_init=PromptTuningInit.RANDOM,  #The added virtual tokens are initializad with random numbers\n",
    "    num_virtual_tokens=NUM_VIRTUAL_TOKENS, #Number of virtual tokens to be added and trained. \n",
    "    tokenizer_name_or_path=model_name #The pre-trained model. \n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating two Prompt Tuning Models. \n",
    "We will create two identical prompt tuning models using the same pre-trained model and the same config. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4,096 || all params: 559,218,688 || trainable%: 0.0007324504863471229\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "peft_model_prompt = get_peft_model(foundational_model, generation_config)\n",
    "print(peft_model_prompt.print_trainable_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4,096 || all params: 559,218,688 || trainable%: 0.0007324504863471229\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "peft_model_sentences = get_peft_model(foundational_model, generation_config)\n",
    "print(peft_model_sentences.print_trainable_parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cff5bc33-8cfb-4144-8962-9c54362a7faa",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "That's amazing: did you see the reduction in trainable parameters? We are going to train a 0.001% of the paramaters available. \n",
    "\n",
    "Now we are going to create the training arguments, and we will use the same configuration in both trainings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "def create_training_arguments(path, learning_rate=0.0035, epochs=6):\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=path, # Where the model predictions and checkpoints will be written\n",
    "        no_cuda=True, # This is necessary for CPU clusters. \n",
    "        auto_find_batch_size=True, # Find a suitable batch size that will fit into memory automatically \n",
    "        learning_rate= learning_rate, # Higher learning rate than full fine-tuning\n",
    "        num_train_epochs=epochs \n",
    "    )\n",
    "    return training_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "54b78a8f-81f0-44c0-b0bc-dcb14891715f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "working_dir = \"./\"\n",
    "\n",
    "#Is best to store the models in separate folders. \n",
    "#Create the name of the directories where to store the models. \n",
    "output_directory_prompt =  os.path.join(working_dir, \"peft_outputs_prompt\")\n",
    "output_directory_sentences = os.path.join(working_dir, \"peft_outputs_sentences\")\n",
    "\n",
    "#Just creating the directoris if not exist. \n",
    "if not os.path.exists(working_dir):\n",
    "    os.mkdir(working_dir)\n",
    "if not os.path.exists(output_directory_prompt):\n",
    "    os.mkdir(output_directory_prompt)\n",
    "if not os.path.exists(output_directory_sentences):\n",
    "    os.mkdir(output_directory_sentences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to indicate the directory containing the model when creating the TrainingArguments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args_prompt = create_training_arguments(output_directory_prompt, 0.003, NUM_EPOCHS)\n",
    "training_args_sentences = create_training_arguments(output_directory_sentences, 0.003, NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c593deb6-5626-4fd9-89c2-2329e2f9b6e0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Train\n",
    "\n",
    "We will create the trainer Object, one for each model to train.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, DataCollatorForLanguageModeling\n",
    "def create_trainer(model, training_args, train_dataset):\n",
    "    trainer = Trainer(\n",
    "        model=model, # We pass in the PEFT version of the foundation model, bloomz-560M\n",
    "        args=training_args, #The args for the training. \n",
    "        train_dataset=train_dataset, #The dataset used to tyrain the model. \n",
    "        data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False) # mlm=False indicates not to use masked language modeling\n",
    "    )\n",
    "    return trainer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "32e43bcf-23b2-46aa-9cf0-455b83ef4f38",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pere/miniforge3/envs/tfphy11/lib/python3.11/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "You're using a BloomTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='35' max='35' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [35/35 01:48, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=35, training_loss=3.6272729056222097, metrics={'train_runtime': 113.226, 'train_samples_per_second': 2.208, 'train_steps_per_second': 0.309, 'total_flos': 50882985099264.0, 'train_loss': 3.6272729056222097, 'epoch': 5.0})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training first model. \n",
    "trainer_prompt = create_trainer(peft_model_prompt, training_args_prompt, train_sample_prompt)\n",
    "trainer_prompt.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='35' max='35' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [35/35 04:01, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=35, training_loss=3.7272120884486606, metrics={'train_runtime': 243.8732, 'train_samples_per_second': 1.025, 'train_steps_per_second': 0.144, 'total_flos': 61846080847872.0, 'train_loss': 3.7272120884486606, 'epoch': 5.0})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training second model. \n",
    "trainer_sentences = create_trainer(peft_model_sentences, training_args_sentences, train_sample_sentences)\n",
    "trainer_sentences.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In less than 10 minutes we trained 2 different models, with two different missions with a same foundational model as a base. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5a6c8daf-8248-458a-9f6f-14865b4fbd2e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Save models\n",
    "We are going to save the models. These models are ready to be used, as long as we have the pre-trained model from which they were created in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "409df5ce-e496-46d7-be2c-202a463cdc80",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "trainer_prompt.model.save_pretrained(output_directory_prompt)\n",
    "trainer_sentences.model.save_pretrained(output_directory_sentences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fb14e3fd-bbf6-4d56-92c2-51bfe08de72a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Inference\n",
    "\n",
    "You can load the model from the path that you have saved to before, and ask the model to generate text based on our input before!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cc48af16-c117-4019-a31a-ce1c93cd21d4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "loaded_model_prompt = PeftModel.from_pretrained(foundational_model, \n",
    "                                         output_directory_prompt,\n",
    "                                         #device_map='auto', \n",
    "                                         is_trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6b44524b-2ac5-4e74-81e6-c406d4414e42",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I want you to act as a motivational coach.  You can use your own words or phrases.']\n"
     ]
    }
   ],
   "source": [
    "loaded_model_prompt_outputs = get_outputs(loaded_model_prompt, input_prompt)\n",
    "print(tokenizer.batch_decode(loaded_model_prompt_outputs, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we compare both answers something changed. \n",
    "* ***Pretrained Model:** I want you to act as a motivational coach.  Don't be afraid of being challenged.*\n",
    "* ***Fine Tuned Model:** I want you to act as a motivational coach.  You can use this method if you're not sure what your goals are.*\n",
    "We have to keep in mind that we have only trained the model for a few minutes, but they have been enough to obtain a response closer to what we were looking for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model_sentences = PeftModel.from_pretrained(foundational_model, \n",
    "                                         output_directory_sentences,\n",
    "                                         #device_map='auto', \n",
    "                                         is_trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['There two thing that matter: one is the number of stars in a galaxy and another, its size. The star system']\n"
     ]
    }
   ],
   "source": [
    "loaded_model_sentences_outputs = get_outputs(loaded_model_sentences, input_sentences)\n",
    "print(tokenizer.batch_decode(loaded_model_sentences_outputs, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the second model we have a similar result.\n",
    "* ***Pretrained Model:** There two thing that matter: the size and shape of a flower*\n",
    "* ***Fine Tuned Model:** There two thing that matter: one is the weather and another, what you do.*\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "Prompt Tuning is an amazing technique than can save us hours of training and a big amount of money. In the notebook we have trained two models in just few minutes and we can have both models in memory giving service to different clients. \n",
    "\n",
    "If you want to try different combinations and models the notebook is ready to use another model from the Bloom family. \n",
    "\n",
    "Yo can change in the third cell the Epochs to train, the num of virtual tokens and the model. But there are a lot of configurations to change, if you want a good exercise can be change the Random initiation of the virtual tokens by a fixed value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "LLM 02 - Prompt Tuning with PEFT",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
