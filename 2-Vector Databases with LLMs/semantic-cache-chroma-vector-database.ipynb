{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1428159,"sourceType":"datasetVersion","datasetId":836401},{"sourceId":6104553,"sourceType":"datasetVersion","datasetId":3496946},{"sourceId":7708579,"sourceType":"datasetVersion","datasetId":1977878}],"dockerImageVersionId":30527,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\n<div align=\"center\">\n<h1><a href=\"https://github.com/peremartra/Large-Language-Model-Notebooks-Course\">Learn by Doing LLM Projects</a></h1>\n    <h3>Understand And Apply Large Language Models</h3>\n    <h2>IMPLEMENTING SEMANTIC CACHE TO IMPROVE A RAG SYSTEM</h2>\n    by <b>Pere Martra</b>\n</div>\n\n<br>\n\n<div align=\"center\">\n    &nbsp;\n    <a target=\"_blank\" href=\"https://www.linkedin.com/in/pere-martra/\"><img src=\"https://img.shields.io/badge/style--5eba00.svg?label=LinkedIn&logo=linkedin&style=social\"></a>\n    \n</div>\n\n<br>\n<hr>\n","metadata":{"execution":{"iopub.status.busy":"2023-10-25T22:11:51.956945Z","iopub.execute_input":"2023-10-25T22:11:51.957387Z","iopub.status.idle":"2023-10-25T22:11:51.965701Z","shell.execute_reply.started":"2023-10-25T22:11:51.957351Z","shell.execute_reply":"2023-10-25T22:11:51.964581Z"}}},{"cell_type":"markdown","source":"### This notebook is part of a comprehensive course on Large Language Models available on GitHub: https://github.com/peremartra/Large-Language-Model-Notebooks-Course. If you want to stay informed about new lessons or updates, simply follow or star the repository.\n\nIn this notebook, we will explore a typical RAG system where we will utilize an open-source model and the vector database Chroma DB. However, we will integrate a semantic cache system that will store various user queries and decide whether to generate the prompt enriched with information from the vector database or the cache.\n\nThe semantic comparison will be performed using the Euclidean distance of question embeddings. This is because, semantically, \"What is the capital of France?\" is essentially the same as \"Tell me the name of the capital of France?\"\n\nTherefore, even though the model's response may vary due to the request for a short answer in the second question, the information to retrieve from the vector database will be the same. This places the cache system between the user and the database, not between the user and the Large Language Model. \n\n<img src=\"https://github.com/peremartra/Large-Language-Model-Notebooks-Course/blob/main/img/semantic_cache.jpg?raw=true\">\n\n### Feel Free to fork or edit the noteboook for you own convenience. Please consider ***UPVOTING IT***. It helps others to discover the notebook, and it encourages me to continue publishing.","metadata":{}},{"cell_type":"markdown","source":"# Import and load the libraries. \nTo start we need to install the necesary Python packages. \n* **[sentence transformers](http:/www.sbert.net/)**. This library is necessary to transform the sentences into fixed-length vectors, also know as embeddings. \n* **[xformers](https://github.com/facebookresearch/xformers)**. it's a package that provides libraries an utilities to facilitate the work with transformers models. We need to install in order to avoid an error when we work with the model and embeddings.  \n* **[chromadb](https://www.trychroma.com/)**. This is our vector Database. ChromaDB is easy to use and open source, maybe the most used Vector Database used to store embeddings. ","metadata":{}},{"cell_type":"code","source":"!pip install -q transformers==4.38.1\n!pip install -q accelerate","metadata":{"execution":{"iopub.status.busy":"2024-02-27T18:45:08.0311Z","iopub.execute_input":"2024-02-27T18:45:08.031491Z","iopub.status.idle":"2024-02-27T18:45:30.383673Z","shell.execute_reply.started":"2024-02-27T18:45:08.031455Z","shell.execute_reply":"2024-02-27T18:45:30.382523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -q sentence-transformers==2.2.2\n!pip install -q xformers==0.0.23\n!pip install -q chromadb==0.4.20","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-02-27T18:45:35.482814Z","iopub.execute_input":"2024-02-27T18:45:35.48358Z","iopub.status.idle":"2024-02-27T18:46:11.405195Z","shell.execute_reply.started":"2024-02-27T18:45:35.483542Z","shell.execute_reply":"2024-02-27T18:46:11.404029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I'm sure that you know the next two packages: Numpy and Pandas, maybe the most used python libraries.\n\nNumpy is a powerful library for numerical computing. \n\nPandas is a library for data manipulation","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd","metadata":{"execution":{"iopub.status.busy":"2024-02-27T18:47:16.231828Z","iopub.execute_input":"2024-02-27T18:47:16.232208Z","iopub.status.idle":"2024-02-27T18:47:16.236908Z","shell.execute_reply.started":"2024-02-27T18:47:16.232174Z","shell.execute_reply":"2024-02-27T18:47:16.235913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load the Dataset\nAs you can see the notebook is ready to work with three different Datasets. Just uncomment the lines of the Dataset you want to use. \n\nI selected Datasets with News. Two of them have just a brief decription of the new, but the other contains the full text. \n\nAs we are working in a free and limited space, and we can use just 30 gb of memory I limited the number of news to use with the variable MAX_NEWS. \n\nThe name of the field containing the text of the new is stored in the variable *DOCUMENT* and the metadata in *TOPIC*","metadata":{}},{"cell_type":"code","source":"#news = pd.read_csv('/kaggle/input/topic-labeled-news-dataset/labelled_newscatcher_dataset.csv', sep=';')\n#MAX_NEWS = 1000\n#DOCUMENT=\"title\"\n#TOPIC=\"topic\"\n\n#news = pd.read_csv('/kaggle/input/bbc-news/bbc_news.csv')\n#MAX_NEWS = 1000\n#DOCUMENT=\"description\"\n#TOPIC=\"title\"\n\nnews = pd.read_csv('/kaggle/input/mit-ai-news-published-till-2023/articles.csv')\nMAX_NEWS = 1000\nDOCUMENT=\"Article Body\"\nTOPIC=\"Article Header\"\n","metadata":{"execution":{"iopub.status.busy":"2024-02-27T18:47:18.177819Z","iopub.execute_input":"2024-02-27T18:47:18.178182Z","iopub.status.idle":"2024-02-27T18:47:18.386774Z","shell.execute_reply.started":"2024-02-27T18:47:18.178152Z","shell.execute_reply":"2024-02-27T18:47:18.385799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"ChromaDB requires that the data has a unique identifier. We can make it with this statement, which will create a new column called **Id**.\n","metadata":{}},{"cell_type":"code","source":"news[\"id\"] = news.index\nnews.head()","metadata":{"execution":{"iopub.status.busy":"2024-02-27T18:47:20.956824Z","iopub.execute_input":"2024-02-27T18:47:20.957501Z","iopub.status.idle":"2024-02-27T18:47:20.984292Z","shell.execute_reply.started":"2024-02-27T18:47:20.957467Z","shell.execute_reply":"2024-02-27T18:47:20.983412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Because it is just a course we select a small portion of News.\nsubset_news = news.head(MAX_NEWS)","metadata":{"execution":{"iopub.status.busy":"2024-02-27T18:47:22.885196Z","iopub.execute_input":"2024-02-27T18:47:22.88598Z","iopub.status.idle":"2024-02-27T18:47:22.890024Z","shell.execute_reply.started":"2024-02-27T18:47:22.885947Z","shell.execute_reply":"2024-02-27T18:47:22.8891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Import and configure the Vector Database\nI'm going to use ChromaDB, the most popular OpenSource vector Database. \n\nFirst we need to import ChromaDB, and after that import the **Settings** class from **chromadb.config** module. This class allows us to change the setting for the ChromaDB system, and customize its behavior. ","metadata":{}},{"cell_type":"code","source":"import chromadb\nfrom chromadb.config import Settings","metadata":{"execution":{"iopub.status.busy":"2024-02-27T18:47:26.146278Z","iopub.execute_input":"2024-02-27T18:47:26.146628Z","iopub.status.idle":"2024-02-27T18:47:26.876245Z","shell.execute_reply.started":"2024-02-27T18:47:26.146599Z","shell.execute_reply":"2024-02-27T18:47:26.875464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we need to create the seetings object calling the **Settings** function imported previously. We store the object in the variable **settings_chroma**.\n\nIs necessary to inform two parameters \n* chroma_db_impl. Here we specify the database implementation and the format how store the data. I choose ***duckdb***, because his high-performace. It operate primarly in memory. And is fully compatible with SQL. The store format ***parquet*** is good for tabular data. With good compression rates and performance. \n\n* persist_directory: It just contains the directory where the data will be stored. Is possible work without a directory and the data will be stored in memory without persistece, but Kaggle dosn't support that. ","metadata":{}},{"cell_type":"code","source":"chroma_client = chromadb.PersistentClient(path=\"/path/to/persist/directory\")","metadata":{"execution":{"iopub.status.busy":"2024-02-27T18:47:28.910404Z","iopub.execute_input":"2024-02-27T18:47:28.910792Z","iopub.status.idle":"2024-02-27T18:47:29.393913Z","shell.execute_reply.started":"2024-02-27T18:47:28.910757Z","shell.execute_reply":"2024-02-27T18:47:29.393179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Filling and Querying the ChromaDB Database\nThe Data in ChromaDB is stored in collections. If the collection exist we need to delete it. \n\nIn the next lines, we are creating the collection by calling the ***create_collection*** function in the ***chroma_client*** created above.","metadata":{}},{"cell_type":"code","source":"collection_name = \"news_collection\"\nif len(chroma_client.list_collections()) > 0 and collection_name in [chroma_client.list_collections()[0].name]:\n        chroma_client.delete_collection(name=collection_name)\n\ncollection = chroma_client.create_collection(name=collection_name)\n    ","metadata":{"execution":{"iopub.status.busy":"2024-02-27T18:47:31.351319Z","iopub.execute_input":"2024-02-27T18:47:31.351661Z","iopub.status.idle":"2024-02-27T18:47:31.421856Z","shell.execute_reply.started":"2024-02-27T18:47:31.351636Z","shell.execute_reply":"2024-02-27T18:47:31.420971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It's time to add the data to the collection. Using the function ***add*** we need to inform, at least ***documents***, ***metadatas*** and ***ids***. \n* In the **document** we store the big text, it's a different column in each Dataset. \n* In **metadatas**, we can informa a list of topics. \n* In **id** we need to inform an unique identificator for each row. It MUST be unique! I'm creating the ID using the range of MAX_NEWS. \n","metadata":{}},{"cell_type":"code","source":"collection.add(\n    documents=subset_news[DOCUMENT].tolist(),\n    metadatas=[{TOPIC: topic} for topic in subset_news[TOPIC].tolist()],\n    ids=[f\"id{x}\" for x in range(MAX_NEWS)],\n)","metadata":{"execution":{"iopub.status.busy":"2024-02-27T18:47:33.708687Z","iopub.execute_input":"2024-02-27T18:47:33.709014Z","iopub.status.idle":"2024-02-27T18:48:29.391684Z","shell.execute_reply.started":"2024-02-27T18:47:33.708989Z","shell.execute_reply":"2024-02-27T18:48:29.39073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def query_database(query_text, n_results=10):\n    results = collection.query(query_texts=query_text, n_results=n_results )\n    return results","metadata":{"execution":{"iopub.status.busy":"2024-02-27T18:48:38.382591Z","iopub.execute_input":"2024-02-27T18:48:38.382965Z","iopub.status.idle":"2024-02-27T18:48:38.387716Z","shell.execute_reply.started":"2024-02-27T18:48:38.382937Z","shell.execute_reply":"2024-02-27T18:48:38.386755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Creating the semantic cache system\nTo implement the cache system, we will use Faiss, a library that allows storing embeddings in memory. It's quite similar to what Chroma does, but without its persistence.\n\nFor this purpose, we will create a class called semantic_cache that will work with its own encoder and provide the necessary functions for the user to perform queries.\n\nIn this class, we first query Faiss (the cache), and if the returned results are above the specified threshold, it will return the result from the cache. Otherwise, it will fetch the result from the Chroma database.","metadata":{}},{"cell_type":"code","source":"!pip install -q faiss-cpu","metadata":{"execution":{"iopub.status.busy":"2024-02-27T18:48:40.919169Z","iopub.execute_input":"2024-02-27T18:48:40.920058Z","iopub.status.idle":"2024-02-27T18:48:52.925059Z","shell.execute_reply.started":"2024-02-27T18:48:40.920022Z","shell.execute_reply":"2024-02-27T18:48:52.923995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import faiss\nfrom sentence_transformers import SentenceTransformer\nimport time\nimport json\n\nclass semantic_cache:\n    def __init__(self, json_file='cache.json'):\n        # Initialize Faiss index with Euclidean distance\n        self.index = faiss.IndexFlatL2(768)  # Use IndexFlatL2 with Euclidean distance\n        if self.index.is_trained:\n            print('Index trained')\n\n        # Initialize Sentence Transformer model\n        self.encoder = SentenceTransformer('all-mpnet-base-v2')\n        self.MAX_SIZE_CACHE = 100\n\n\n        # Uncomment the following lines to use DialoGPT for question generation\n        # self.tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-large\")\n        # self.model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-large\")\n\n        # Set Euclidean distance threshold\n        self.euclidean_threshold = 0.7\n        self.json_file = json_file\n        self.load_cache()\n    \n    def load_cache(self):\n        # Load cache from JSON file, creating an empty cache if the file is not found\n        try:\n            with open(self.json_file, 'r') as file:\n                self.cache = json.load(file)\n        except FileNotFoundError:\n            self.cache = {'questions': [], 'embeddings': [], 'answers': [], 'response_text': []}\n\n    def save_cache(self):\n        # Save the cache to the JSON file\n        with open(self.json_file, 'w') as file:\n            json.dump(self.cache, file)\n            \n    def generate_answer(self, question: str) -> str:\n        # Method to generate an answer using a separate function (make_prediction in this case)\n        try:\n            #result = make_prediction([question])\n            result = query_database([question], 1)\n            response_text = result['documents'][0][0]\n\n            return result, response_text\n        except Exception as e:\n            raise RuntimeError(f\"Error during 'generate_answer' method: {e}\")\n    \n    def ask(self, question: str) -> str:\n        # Method to retrieve an answer from the cache or generate a new one\n        start_time = time.time()\n        try:\n            l = [question]\n            embedding = self.encoder.encode(l)\n\n            # Search for the nearest neighbor in the index\n            D, I = self.index.search(embedding, 1)\n\n            if D[0] >= 0:\n                if I[0][0] != -1 and D[0][0] <= self.euclidean_threshold:\n                    row_id = int(I[0][0])\n                    print(f'Found cache in row: {row_id} with score {1 - D[0][0]}')\n                    end_time = time.time()\n                    elapsed_time = end_time - start_time\n                    print(f\"Time taken: {elapsed_time} seconds\")\n                    return self.cache['response_text'][row_id]\n\n            # Handle the case when there are not enough results or Euclidean distance is not met\n            answer, response_text = self.generate_answer(question)\n            \n            if len(self.cache[\"questions\"]) == self.MAX_SIZE_CACHE:\n                self.cache[\"questions\"].pop(0)\n                self.cache[\"embeddings\"].pop(0)\n                self.cache[\"answers\"].pop(0)\n                self.cache[\"response_text\"].pop(0)\n\n            self.cache['questions'].append(question)\n            self.cache['embeddings'].append(embedding[0].tolist())\n            self.cache['answers'].append(answer)\n            self.cache['response_text'].append(response_text)\n\n            self.index.add(embedding)\n            self.save_cache()\n            end_time = time.time()\n            elapsed_time = end_time - start_time\n            print(f\"Time taken: {elapsed_time} seconds\")\n\n            return response_text\n        except Exception as e:\n            raise RuntimeError(f\"Error during 'ask' method: {e}\")\n            \n","metadata":{"execution":{"iopub.status.busy":"2024-02-27T18:48:52.927719Z","iopub.execute_input":"2024-02-27T18:48:52.92815Z","iopub.status.idle":"2024-02-27T18:48:57.166591Z","shell.execute_reply.started":"2024-02-27T18:48:52.92811Z","shell.execute_reply":"2024-02-27T18:48:57.165783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cache = semantic_cache()","metadata":{"execution":{"iopub.status.busy":"2024-02-27T18:48:57.167736Z","iopub.execute_input":"2024-02-27T18:48:57.168287Z","iopub.status.idle":"2024-02-27T18:49:03.383809Z","shell.execute_reply.started":"2024-02-27T18:48:57.168257Z","shell.execute_reply":"2024-02-27T18:49:03.382615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"question1 = \"recent investigations about LLMs\"\nresults = cache.ask(question1)","metadata":{"execution":{"iopub.status.busy":"2024-02-27T18:49:03.386334Z","iopub.execute_input":"2024-02-27T18:49:03.386641Z","iopub.status.idle":"2024-02-27T18:49:04.653282Z","shell.execute_reply.started":"2024-02-27T18:49:03.386614Z","shell.execute_reply":"2024-02-27T18:49:04.652009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(results)","metadata":{"execution":{"iopub.status.busy":"2024-02-27T18:49:04.654566Z","iopub.execute_input":"2024-02-27T18:49:04.65654Z","iopub.status.idle":"2024-02-27T18:49:04.670371Z","shell.execute_reply.started":"2024-02-27T18:49:04.656509Z","shell.execute_reply":"2024-02-27T18:49:04.668862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"question1 = \"What is Monica Agrawal currently working on?\"\nresults = cache.ask(question1)","metadata":{"execution":{"iopub.status.busy":"2024-02-27T18:49:04.672916Z","iopub.execute_input":"2024-02-27T18:49:04.673209Z","iopub.status.idle":"2024-02-27T18:49:05.97694Z","shell.execute_reply.started":"2024-02-27T18:49:04.673183Z","shell.execute_reply":"2024-02-27T18:49:05.976039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(results)","metadata":{"execution":{"iopub.status.busy":"2024-02-27T18:49:05.978428Z","iopub.execute_input":"2024-02-27T18:49:05.978915Z","iopub.status.idle":"2024-02-27T18:49:05.983483Z","shell.execute_reply.started":"2024-02-27T18:49:05.978888Z","shell.execute_reply":"2024-02-27T18:49:05.98256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"question1 = \"Can you tell me about Monica Agrawal's current projects?\"\nresults = cache.ask(question1)","metadata":{"execution":{"iopub.status.busy":"2024-02-27T18:49:05.984624Z","iopub.execute_input":"2024-02-27T18:49:05.984891Z","iopub.status.idle":"2024-02-27T18:49:06.025605Z","shell.execute_reply.started":"2024-02-27T18:49:05.984867Z","shell.execute_reply":"2024-02-27T18:49:06.024628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(results)","metadata":{"execution":{"iopub.status.busy":"2024-02-27T18:49:06.026808Z","iopub.execute_input":"2024-02-27T18:49:06.027074Z","iopub.status.idle":"2024-02-27T18:49:06.032183Z","shell.execute_reply.started":"2024-02-27T18:49:06.02705Z","shell.execute_reply":"2024-02-27T18:49:06.031278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Once we have our information inside the Database we can query It, and ask for data that matches our needs. The search is done inside the content of the document, and it dosn't look for the exact word, or phrase. The results will be based on the similarity between the search terms and the content of documents. \n\nThe metadata is not used in the search, but they can be utilized for filtering or refining the results after the initial search. \n","metadata":{}},{"cell_type":"markdown","source":"# Loading the model and creating the prompt\nTRANSFORMERS!!\nTime to use the library **transformers**, the most famous library from [hugging face](https://huggingface.co/) for working with language models. \n\nWe are importing: \n* **Autotokenizer**: It is a utility class for tokenizing text inputs that are compatible with various pre-trained language models.\n* **AutoModelForCasualLLM**: it provides an interface to pre-trained language models specifically designed for language generation tasks using causal language modeling (e.g., GPT models), or the model used in this notebook ***databricks/dolly-v2-3b***.\n* **pipeline**: provides a simple interface for performing various natural language processing (NLP) tasks, such as text generation (our case) or text classification. \n\nThe model selected is [dolly-v2-3b](https://huggingface.co/databricks/dolly-v2-3b), the smallest Dolly model. It have 3billion paramaters, more than enough for our sample, and works much better than GPT2. \n\nPlease, feel free to test [different Models](https://huggingface.co/models?pipeline_tag=text-generation&sort=trending), you need to search for NLP models trained for text-generation. My recomendation is choose \"small\" models, or we will run out of memory in kaggle.  \n","metadata":{}},{"cell_type":"code","source":"from getpass import getpass\nhf_key = getpass(\"Hugging Face Key: \")","metadata":{"execution":{"iopub.status.busy":"2024-02-27T18:52:46.735019Z","iopub.execute_input":"2024-02-27T18:52:46.735663Z","iopub.status.idle":"2024-02-27T18:52:59.287919Z","shell.execute_reply.started":"2024-02-27T18:52:46.735631Z","shell.execute_reply":"2024-02-27T18:52:59.286929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!huggingface-cli login --token $hf_key","metadata":{"execution":{"iopub.status.busy":"2024-02-27T18:53:03.061336Z","iopub.execute_input":"2024-02-27T18:53:03.062041Z","iopub.status.idle":"2024-02-27T18:53:04.685113Z","shell.execute_reply.started":"2024-02-27T18:53:03.062006Z","shell.execute_reply":"2024-02-27T18:53:04.683932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#In a MAC Silicon the device must be 'mps'\n# device = torch.device('mps') #to use with MAC Silicon\ndevice = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'","metadata":{"execution":{"iopub.status.busy":"2024-02-27T18:53:06.981179Z","iopub.execute_input":"2024-02-27T18:53:06.98213Z","iopub.status.idle":"2024-02-27T18:53:07.410443Z","shell.execute_reply.started":"2024-02-27T18:53:06.982089Z","shell.execute_reply":"2024-02-27T18:53:07.409245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n\n#model_id = \"databricks/dolly-v2-3b\"\nmodel_id = \"google/gemma-2b\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nlm_model = AutoModelForCausalLM.from_pretrained(model_id)","metadata":{"execution":{"iopub.status.busy":"2024-02-27T18:53:07.675549Z","iopub.execute_input":"2024-02-27T18:53:07.676412Z","iopub.status.idle":"2024-02-27T18:53:36.690337Z","shell.execute_reply.started":"2024-02-27T18:53:07.676374Z","shell.execute_reply":"2024-02-27T18:53:36.689215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The next step is to initialize the pipeline using the objects created above. \n\nThe model's response is limited to 256 tokens, for this project I'm not interested in a longer response, but it can easily be extended to whatever length you want.\n\nSetting ***device_map*** to ***auto*** we are instructing the model to automaticaly select the most appropiate device: CPU or GPU for processing the text generation.  ","metadata":{}},{"cell_type":"code","source":"pipe = pipeline(\n    \"text-generation\",\n    model=lm_model,\n    tokenizer=tokenizer,\n    max_new_tokens=256,\n    device_map=\"auto\",\n)","metadata":{"execution":{"iopub.status.busy":"2024-02-27T19:05:16.204981Z","iopub.execute_input":"2024-02-27T19:05:16.205942Z","iopub.status.idle":"2024-02-27T19:05:16.214111Z","shell.execute_reply.started":"2024-02-27T19:05:16.205904Z","shell.execute_reply":"2024-02-27T19:05:16.21328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Creating the extended prompt\nTo create the prompt we use the result from query the Vector Database  and the sentence introduced by the user. \n\nThe prompt have two parts, the **relevant context** that is the information recovered from the database and the **user's question**. \n\nWe only need to join the two parts together to create the prompt that we are going to send to the model. \n\nYou can limit the lenght of the context passed to the model, because we can get some Memory problems with one of the datasets that contains a realy large text in the document part. ","metadata":{}},{"cell_type":"code","source":"question = \"Can you tell me about Monica Agrawal's current projects?\"\n#context = \" \".join([f\"#{str(i)}\" for i in results[\"documents\"][0]])\n#context = context[0:5120]\nprompt_template = f\"Relevant context: {results}\\n\\n The user's question: {question}\"\nprompt_template","metadata":{"execution":{"iopub.status.busy":"2024-02-27T19:05:19.545721Z","iopub.execute_input":"2024-02-27T19:05:19.546082Z","iopub.status.idle":"2024-02-27T19:05:19.552819Z","shell.execute_reply.started":"2024-02-27T19:05:19.546052Z","shell.execute_reply":"2024-02-27T19:05:19.55184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now all that remains is to send the prompt to the model and wait for its response!\n","metadata":{}},{"cell_type":"code","source":"lm_response = pipe(prompt_template)\nprint(lm_response[0][\"generated_text\"])","metadata":{"execution":{"iopub.status.busy":"2024-02-27T19:05:27.357926Z","iopub.execute_input":"2024-02-27T19:05:27.358798Z","iopub.status.idle":"2024-02-27T19:10:41.33629Z","shell.execute_reply.started":"2024-02-27T19:05:27.358737Z","shell.execute_reply":"2024-02-27T19:10:41.335292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusions, Fork and Improve\nA very short notebook, but with a lot of content.\n\nWe have used a vector database to store information. Then move on to retrieve it and use it to create an extended prompt that we've used to call one of the newer large language models available in Hugging Face.\n\nThe model has returned a response to us taking into account the context that we have passed to it in the prompt.\n\nThis way of working with language models is very powerful.\n\nWe can make the model use our information without the need for Fine Tuning. This technique really has some very big advantages over fine tuning.\n\nPlease don't stop here.\n\n* The notebook is prepared to use two more Datasets. Do tests with it.\n\n* Find another model on Hugging Face and compare it.\n\n* Modify the way to create the prompt.\n\n## Continue learning\nThis notebook is part of a [course on large language models](https://github.com/peremartra/Large-Language-Model-Notebooks-Course) I'm working on and it's available on [GitHub](https://github.com/peremartra/Large-Language-Model-Notebooks-Course). You can see the other lessons and if you like it, don't forget to subscribe to receive notifications of new lessons.\n\nOther notebooks in the Large Language Models series: \nhttps://www.kaggle.com/code/peremartramanonellas/ask-your-documents-with-langchain-vectordb-hf","metadata":{"execution":{"iopub.status.busy":"2023-07-12T22:01:56.992775Z","iopub.execute_input":"2023-07-12T22:01:56.993351Z","iopub.status.idle":"2023-07-12T22:01:57.001309Z","shell.execute_reply.started":"2023-07-12T22:01:56.993305Z","shell.execute_reply":"2023-07-12T22:01:56.999431Z"}}},{"cell_type":"markdown","source":"### If you liked the notebook Please consider ***UPVOTING IT***. It helps others to discover it, and encourages me to continue publishing.","metadata":{"execution":{"iopub.status.busy":"2023-07-12T22:17:34.63605Z","iopub.execute_input":"2023-07-12T22:17:34.636604Z","iopub.status.idle":"2023-07-12T22:17:34.644497Z","shell.execute_reply.started":"2023-07-12T22:17:34.636566Z","shell.execute_reply":"2023-07-12T22:17:34.642819Z"}}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}